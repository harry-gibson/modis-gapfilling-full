{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook contains a Python / Cython implementation of code used to gap-fill images derived from the MODIS data catalogue, specifically LST Day / Night, EVI, TCB, and TCW.\n",
    "\n",
    "The original algorithms used were developed by Dan Weiss and are described in the following paper:\n",
    "\n",
    "Weiss, D.J., Atkinson, P.M., Bhatt, S., Mappin, B., Hay, S.I. & Gething, P.W. (2014) An effective approach for gap-filling continental scale remotely sensed time-series. ISPRS Journal of Photogrammetry and Remote Sensing, 98, 106-118\n",
    "\n",
    "[doi:10.1016/j.isprsjprs.2014.10.001](http://dx.doi.org/10.1016/j.isprsjprs.2014.10.001)\n",
    "\n",
    "The code herein is a reimplementation of the Weiss code (which was written in IDL) by Harry Gibson, also at the [Malaria Atlas Project](http://www.map.ox.ac.uk/), Oxford. \n",
    "\n",
    "The core algorithms are unchanged from the published paper but substantial improvements in efficiency have been made enabling the (more effective but more computationally demanding) A1 algorithm to be used for larger (thus, more) gaps, whilst still allowing gapfilling of 8-daily global 1km MODIS data stacks to be undertaken in a reasonable period. Running with 20 cores on a Blade server, total runtime on a 15-year 8-daily 1km global stack was in the region of 5-6 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code including the algorithms and calls necessary to run them on a series of images is included in this notebook. This means that the Cython code can be compiled and run very easily, although it does mean that the notebook is rather long! \n",
    "\n",
    "Cells containing code to configure the run come first. These should be adjusted to suit your data and needs, and executed. \n",
    "\n",
    "Next comes a cell showing how the algorithms should actually be called. Before running this cell, you should first run all the cells containing the actual model code and utility functions - these come below the header \"Operational Code\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the algorithms have been made more efficient, a high-specification machine with plenty of RAM is still required to fill global images. \n",
    "\n",
    "We can run despeckle and A1 in slices that fit into memory, given the need for A1 to have a stack of (currently) 15 calendar-day images, plus the same of outputs and distances and flags. \n",
    "\n",
    "We can run a slice of approximately 10k pixels width * full height on a 64Gb machine - but A2 needs more than this as it cannot be run by slices and in the current implementation needs all 8 passes to be kept in order to calculate median values at each point (using means would get around this). \n",
    "\n",
    "Writing the intermediates to disk is a substantial overhead when there is not much other free memory available, but unavoidable. \n",
    "\n",
    "Also the OS does not free the memory until the disk has actually written it, if disk write caching is enabled. This means it can be beneficial to have a slightly compressed format for the intermediates, else A2 will not have as much memory as it should have. 15 days * 43200 * 21600 pixels * 4 bytes per pixel * 2.25 (out,dists,flags) = 117Gb!\n",
    "\n",
    "**Bottom line - for global 1k fills a machine with at least 100Gb RAM would be recommended!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually ran on MAP's blade servers (64 cores, 500Gb RAM). It could therefore run without saving intermediate data to disk. For A1 this would require memory for the full 15 year stack plus 8 A2 directional images, and mean and std; this would be in the region of 100Gb.\n",
    "\n",
    "In fact I ran with memory use set to ~70Gb which meant that the A1 processed by slices with intermediate files; this did not prove significantly slower (presumably the OS cached those intermediate files) but kept the machine more usable for others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences from published algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having been parallelised and substantially optimised, the despeckle and fill algorithms are fundamentally mostly unchanged from those developed by Dan Weiss and described in Weiss et al. Specific changes that were made are noted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill value calculation approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gapfilling, we originally (in the published paper) calculated ratios between values occurring at different times.\n",
    "\n",
    "However this method only makes sense if the scale is ratiometric. \n",
    "For temperature in celsius that's not the case: 10 degrees is not meaningfully twice\n",
    "as hot as five degrees. Five degrees is not infinitely warmer than zero degrees.\n",
    "Minus five degrees is not minus one times as warm as five degrees. \n",
    "\n",
    "For temperature we can use an absolute scale of kelvin: the higher numbers means that \n",
    "the ratio is going to be closer to 1 but it's multiplied by a bigger alternate value\n",
    "too so it is justifiable.\n",
    "\n",
    "EVI is an absolute (ratio) scale, but we need to handle the case when\n",
    "either point is zero or very close to it, so we need to define a maximum plausible ratio\n",
    "so if a point is 0.01 one year then 0.9 another year we don't bring in a huge multiplier of the \n",
    "alternate value point.\n",
    "\n",
    "For tasseled cap brightness the coefficients are all positive, so treat as EVI. \n",
    "\n",
    "For tc wetness, the values the values can be positive or negative. But I think we \n",
    "need to transform these to make them all positive as it's not really an absolute \n",
    "scale; the numbers don't really mean anything; so TCW needs to be handled like LST \n",
    "but specify a max ratio too.\n",
    "\n",
    "The alternative is to examine the _difference_ between values rather than the _ratio_ in the algorithms. This is safer, and the only real assumption is that the scale is linear (or it could be linearised). \n",
    "\n",
    "The code in this notebook offers both options; the difference method was used for all the MODIS global covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 pass averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the published paper, A2 ran 8 directional passes and selected the median value of the results at each location. In this implementation A2 can use either the mean or the median value at each location. Whilst the mean more sensitive to outliers (which is why the median was originally used) this was not observed to be a problem in global tests and the mean was less prone to visual banding. Additionally as a mean can be computed on-the-fly, A2 could be made more memory efficient as the 8 passes do not all need to be stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search ratios and thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generating our global data series we ran A1 with a larger search radius than was used in the published paper. This was possible due to the faster implementation. This means that more gap pixels were filled with A1 than previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean / standard deviation reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original algorithm, despeckle and A2 fills are run by reference to a single all-time \"synoptic\" image for mean and standard deviation values. This was also the case here for most data, but the code has been adapted to allow a \"stack\" of mean and standard deviation images, such as one for each calendar month. This was used in despeckling EVI and other BRDF-derived imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have occasionally observed what seems to be an obscure linux kernel bug that can lead to the \"migration\" process taking almost all CPU time and nothing actually getting done during the multithreaded portions of the gapfill code. I have worked around this by setting CPU affinity for the threads I'm using. \n",
    "\n",
    "But, this needs to be done separately for each IPython kernel else, if running two gapfill sessions, they'll compete for the same cores.\n",
    "\n",
    "So assuming a 64-core machine, for one session use say cores 0 - 20 and then a different 20 for another session, leaving the OS free to schedule other people's work on the remaining cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOMP_CPU_AFFINITY'] = '1-25'\n",
    "\n",
    "# or get rid of that if not needed\n",
    "# os.unsetenv('GOMP_CPU_AFFINITY')\n",
    "# os.environ.pop('GOMP_CPU_AFFINITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports / required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import glob\n",
    "import rasterio\n",
    "import scipy.ndimage\n",
    "import shutil\n",
    "import bottleneck as bn\n",
    "import gc\n",
    "import numexpr as ne\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of flags and thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should the fill be based on the difference between neighbour values or the ratio between them?\n",
    "# Ratio only works for ratiometric variables such as temperature in Kelvin\n",
    "_FILL_BY_RATIO = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are running by ratio then set appropriate values if necessary to put the values into a \"more absolute\" scale e.g. to turn celsius into kelvin. Also set a max ratio in case there are values close to zero (like with EVI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LST\n",
    "# value that could be subtracted from the data to put them into a close-enough ratio scale\n",
    "_DATA_ZERO_OFFSET = -273.15 # just put the values into kelvin which is a ratio scale\n",
    "_MAX_ALLOWABLE_RATIO = 2.0 # will never be seen with LST in kelvin\n",
    "\n",
    "# EVI\n",
    "#_DATA_ZERO_OFFSET = 0.0 # EVI is an absolute scale anyway\n",
    "#_MAX_ALLOWABLE_RATIO = 5.0 # but values close to 0 will give large ratio, so define a limit\n",
    "\n",
    "# TCB \n",
    "# TCB is an arbitrary unit but is always positive, so treat as an absolute scale\n",
    "#_DATA_ZERO_OFFSET = 0.0 # EVI is an absolute scale anyway\n",
    "#_MAX_ALLOWABLE_RATIO = 10.0 # be a bit more lax as i guess it varies more\n",
    "\n",
    "# TCW \n",
    "# TCW is also an arbitrary unit but can be negative or positive so calculate ratios on shifted values\n",
    "#_DATA_ZERO_OFFSET = 3.0 # TCW \n",
    "#_MAX_ALLOWABLE_RATIO = 5.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other data \"fixes\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data were converted wrong from HDF then fix that here: i mistakenly made the celsius grids by subtracting 273.0 from the \n",
    "# original kelvin data so need to remove another 0.15\n",
    "#_DATA_CORRECTION_OFFSET = -0.15\n",
    "_DATA_CORRECTION_OFFSET = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill algorithm thresholds and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRIM_MIN_MAX = 1\n",
    "# Should the fill values be clipped?\n",
    "_CLIP_TO_LIMITS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard upper / lower limits that the fill values should be clipped to\n",
    "# 1-0 for EVI, 100 - -100 for LST\n",
    "#_DATA_UPPER_LIMIT = 1.0\n",
    "#_DATA_LOWER_LIMIT = 0.0\n",
    "#_DATA_UPPER_LIMIT = 2.0\n",
    "#_DATA_LOWER_LIMIT = -1.0\n",
    "\n",
    "_DATA_UPPER_LIMIT = 100.0\n",
    "_DATA_LOWER_LIMIT = -100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despeckle thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of stds beyond the mean beyond which a fill value will be clipped\n",
    "_FLOOR_CEILING_VALUE = 2.58\n",
    "# Number of stds beyond the mean beyond which a value will be unconditionally discarded in despeckle\n",
    "_EXTREME_VALUE_THRESHOLD = 2.58\n",
    "# Number of stds beyond the mean beyond which a value may be discarded in despeckly, depending on neighborhood similarity\n",
    "_SPECKLE_THRESHOLD = 1.64 #1.96\n",
    "# Max difference in Z-score between a potential speckle and the average of its neighbours, for it \n",
    "# to be accepted as not being a speckle\n",
    "_SPECKLE_NBR_Z_THRESH = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spiral search distance / density (-> search radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many cells should the spiral search in despeckle check up to (unless \n",
    "# it finds enough nbrs beforehand)\n",
    "# effective search radius is therefore sqrt(value / pi)\n",
    "_DESPECKLE_SEARCH_NBRS = 3142 #31420\n",
    "# How many cells must be found within the above figure to succeed?\n",
    "_DESPECKLE_MIN_USED_NBRS = 320 #1600\n",
    "# How many cells will we go up to before we stop searching, if we find them\n",
    "# before the end of the spiral search is reached?\n",
    "_DESPECKLE_MAX_USED_NBRS = 640 #3200\n",
    "\n",
    "# How many cells should the spiral search in A1 check up to (unless it finds \n",
    "# enough nbrs beforehand)\n",
    "# A1 search radius is therefore approx sqrt (value / pi)\n",
    "_A1_SEARCH_NBRS = 3142\n",
    "# At least how many cells must be found within above num nbrs to succeed?\n",
    "# Bear in mind that the spiral will run (n years - 1) times and this value\n",
    "# applies to the entire search in total\n",
    "_A1_MIN_USED_NBRS = 480# 320#40\n",
    "# Stop the spiral search when we have found how many nbrs?\n",
    "_A1_MAX_USED_NBRS = 960#640#80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1 memory use (-> should we process by tile?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much memory can A1 use? If necessary A1 will be run in slices so it can fit the stack \n",
    "# within the available memory. \n",
    "# A2 can't do this and will take what it needs, which is around 70Gb for global 1k data. \n",
    "# So not much point setting this to less than that.\n",
    "# 190e9 will ensure A1 processes entirely in memory \n",
    "#totalAvailMem = 190e9\n",
    "totalAvailMem = 40e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for saving A1 intermediate data if necessary (can be TABLES or TIFF, use TIFF)\n",
    "_SAVEMETHOD = \"TIFF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we run A2, or only A1 (leaving gaps where A1 fails)\n",
    "_RUN_A2 = 1\n",
    "\n",
    "# Which of the 8 passes should A2 choose? the mean or median of the non-nan results?\n",
    "_A2_PASS_SELECTOR = \"MEAN\" # or MEDIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag values to use. They go into an 8 bit bitmask value and are set/unset with \n",
    "# bitwise operators so don't change these without thought!!\n",
    "_OCEAN_FLAG = 1\n",
    "_FILL_FAILED_FLAG = 2 # indicates that whatever fill algorithm ran last failed\n",
    "_EXTREME_FLAG = 4\n",
    "_SPECKLE_FLAG = 8\n",
    "_A1_FILLED_FLAG = 16\n",
    "_A1_FILL_WAS_FULL_FLAG = 32\n",
    "_A2_SUCCESS_FLAG = 64\n",
    "_MINMAX_CLIP_FLAG = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AOI for gapfill and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAOI(WestLimit, EastLimit, NorthLimit, SouthLimit):\n",
    "    ''' Gets the pixel coordinates on a global 30 arcsecond image for the given bbox in degrees'''\n",
    "    resolution = 0.008333333333333 # 12 threes\n",
    "    assert EastLimit > WestLimit\n",
    "    assert NorthLimit > SouthLimit\n",
    "    \n",
    "    x0 = int((WestLimit - -180) / resolution)\n",
    "    x1 = int(((EastLimit - -180) / resolution)+0.5)\n",
    "    y0 = int((90 - NorthLimit) / resolution)\n",
    "    y1 = int(((90 - SouthLimit) / resolution) + 0.5)\n",
    "    \n",
    "    return ((x0,x1),(y0,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not adjust these : they are the size of the WGS84 global 1km tiffs\n",
    "sourceDataHeight = 21600\n",
    "sourceDataWidth = 43200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the input files are all 1km global. We may not want to gapfill the whole lot. Configure that here: use CalculateAOI() to get the necessary values based on lat / long boxes e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalculateAOI(-2,40,15,-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust these as shown below if a non-global AOI is required\n",
    "xLims = (0,43200)\n",
    "yLims = (0,21600)\n",
    "\n",
    "# Mountainous EVI shadow speckles area\n",
    "#xLims = (29160, 31560)\n",
    "#yLims = (5400, 7200)\n",
    "\n",
    "# Eastern China dodgy LST test area\n",
    "#xLims = (33600,36600)\n",
    "#yLims = (6000,9000)\n",
    "\n",
    "#Africa (Same as original algorithm cubes):\n",
    "#xLims = (18467, 18467+12532)\n",
    "#yLims = (6000, 6000+9600)\n",
    "\n",
    "#African cloudy Random Test Area, 2-12W, 12-2N\n",
    "#xLims = (21840,23040)\n",
    "#yLims = (9660,10560)\n",
    "\n",
    "# All but Antarctica:\n",
    "#xLims = (0,43200)\n",
    "#yLims = (0, 18000)\n",
    "\n",
    "# E8 nations\n",
    "#xLims = (22800, 26640)\n",
    "#yLims = (11280, 15120)\n",
    "\n",
    "# Haiti\n",
    "# xLims = (12600, 13080)\n",
    "# yLims = (8280, 8760)\n",
    "\n",
    "# Testing\n",
    "xLims = (21360, 26400)\n",
    "yLims = (9000, 12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sources configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the wildcard path to the input data files, and the synoptic OR stacked mean and std files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strPattern gives the wildcarded path to the input data files, where {} can be replaced \n",
    "# with a calendar day number to list all tif files of that calendar day e.g. A2014009_Day.tif\n",
    "# means_Synoptic_Path should be the \"balanced mean\" file i.e. the mean of the monthly means\n",
    "# std_Synoptic_Path should be the daily sd file, i.e. the s.d. of the individual days\n",
    "\n",
    "# *** EVI ***\n",
    "#strPattern = r'E:\\MCD43B4\\MCD43B4_Indices\\EVI\\*{}_*.tif'\n",
    "#means_Synoptic_Path = r'G:\\NewStats\\EVI_Mean_From_Monthly.tif'\n",
    "#std_Synoptic_Path = r'G:\\NewStats\\EVI_SD_From_Daily.tif'\n",
    "\n",
    "# *** LST DAY ***\n",
    "#strPattern = r'F:\\MOD11A2\\MOD11A2_Data\\Day\\*{}_*.tif'\n",
    "#means_Synoptic_Path = r'G:\\NewStats\\LST_Day_Mean_From_Monthly.tif'\n",
    "#std_Synoptic_Path = r'G:\\NewStats\\LST_Day_SD_From_Daily.tif'\n",
    "\n",
    "# *** LST NIGHT ***\n",
    "#strPattern = r'F:\\MOD11A2\\MOD11A2_Data\\Night\\*{}_*.tif'\n",
    "#means_Synoptic_Path = r'G:\\NewStats\\LST_Night_Mean_From_Monthly.tif'\n",
    "#std_Synoptic_Path = r'G:\\NewStats\\LST_Night_SD_From_Daily.tif'\n",
    "\n",
    "#means_Synoptic_Path = '/home/ZOO/zool1301/Gapfilling/Stats/LST_Day_Month_1_Mean.tif'\n",
    "#std_Synoptic_Path = '/home/ZOO/zool1301/Gapfilling/Stats/LST_Day_Month_1_SD.tif'\n",
    "\n",
    "# *** TCW ***\n",
    "#strPattern = '/home/ZOO/zool1301/Gapfilling/TCW/Input/*{}_*.tif'\n",
    "#means_Synoptic_Path = '/home/ZOO/zool1301/Gapfilling/Stats/TCW_Mean_From_Monthly.tif'\n",
    "#std_Synoptic_Path = '/home/ZOO/zool1301/Gapfilling/Stats/TCW_SD_From_Daily.tif'\n",
    "\n",
    "# *** TCB ***\n",
    "# strPattern = r'L:\\TCW_v6_Unfilled\\1km\\8-Daily\\*{}_TCW.tif'\n",
    "# means_Synoptic_Path = r'L:\\TCW_v6_Unfilled\\1km\\Synoptic\\TCW_Unfilled_V6.Synoptic.Overall.Balanced-mean.1km.Data.tif'\n",
    "# std_Synoptic_Path = r'L:\\TCW_v6_Unfilled\\1km\\Synoptic\\TCW_Unfilled_V6.Synoptic.Overall.SD.1km.Data.tif'\n",
    "\n",
    "# *** TESTING ***\n",
    "strPattern = r'G:\\Shared drives\\MODIS-unfilled\\LST_Day_v6\\1km\\8-Daily\\*{}_LST_Day*.tif'\n",
    "means_Synoptic_Path = r'G:\\Shared drives\\MODIS-unfilled\\LST_Day_v6\\1km\\Synoptic\\LST_Day_v6_Unfilled.Synoptic.Overall.Balanced-mean.mean.1km.tif'\n",
    "std_Synoptic_Path = r'G:\\Shared drives\\MODIS-unfilled\\LST_Day_v6\\1km\\Synoptic\\LST_Day_v6_Unfilled.Synoptic.Overall.SD.1km.Data.tif'\n",
    "\n",
    "means_Stack_Path = None\n",
    "std_Stack_Path = None\n",
    "stdsPreloaded=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the path to the land/sea mask dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coast_Path = r'G:\\Shared drives\\MAP MODIS Covariates\\MODIS_Global\\CoastGlobal.tiff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the output and temp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp dir should be on C or whatever disk is fastest and have ~200Gb or more free space\n",
    "#tempDir = os.path.join(os.environ['TEMP'],\"GapfillTemp\")\n",
    "tempDir = r'C:\\Temp\\LST_Test_NB\\Intermediate'\n",
    "outDir = r'C:\\Temp\\LST_Test_NB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF CONFIG SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values in this section will be calcualted based on previous inputs, or are hard-coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always round up as there's no harm done in passing slightly too much data\n",
    "_A1_SEARCH_RADIUS = int(math.sqrt (_A1_SEARCH_NBRS / 3.14) + 1)\n",
    "_DESPECKLE_SEARCH_RADIUS = int(math.sqrt (_DESPECKLE_SEARCH_NBRS / 3.14) + 1)\n",
    "\n",
    "_TOTAL_REQ_MARGIN = _A1_SEARCH_RADIUS + _DESPECKLE_SEARCH_RADIUS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayMonths = {1:1, 9:1, 17:1, 25:1, 33:2, 41:2, 49:2, 57:2, 65:3, 73:3, 81:3, 89:3, 97:4, 105:4, 113:4, 121:4, \n",
    "             129:5, 137:5, 145:5, 153:6, 161:6, 169:6, 177:6, 185:7, 193:7, 201:7, 209:7, 217:8, 225:8, 233:8, \n",
    "             241:8, 249:9, 257:9, 265:9, 273:9, 281:10, 289:10, 297:10, 305:10, 313:11, 321:11, 329:11, 337:12, \n",
    "             345:12, 353:12, 361:12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendarDays = [str(i).zfill(3) for i in np.arange(1,365,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlagValues = {\n",
    "         \"OCEAN\" : _OCEAN_FLAG,\n",
    "         \"FAILURE\" : _FILL_FAILED_FLAG,\n",
    "         \"SPECKLE\" : _SPECKLE_FLAG,\n",
    "         \"EXTREME\" : _EXTREME_FLAG,\n",
    "         \"A1_FILLED\" : _A1_FILLED_FLAG,\n",
    "         \"A1_FULL\" : _A1_FILL_WAS_FULL_FLAG,\n",
    "         \"A2_FILLED\" : _A2_SUCCESS_FLAG,\n",
    "         \"CLIPPED\" : _MINMAX_CLIP_FLAG\n",
    "         }\n",
    "DespeckleSpiralConfig = {\n",
    "        \"MAX_NBRS_TO_SEARCH\" : _DESPECKLE_SEARCH_NBRS,\n",
    "        \"MIN_NBRS_REQUIRED\" : _DESPECKLE_MIN_USED_NBRS,\n",
    "        \"MAX_NBRS_REQUIRED\" : _DESPECKLE_MAX_USED_NBRS\n",
    "        }\n",
    "DespeckleLimitsConfig = {\n",
    "        \"HARD_UPPER_LIMIT\" : _DATA_UPPER_LIMIT,\n",
    "        \"HARD_LOWER_LIMIT\" : _DATA_LOWER_LIMIT,\n",
    "        \"INVALID_BEYOND_STDS\": _EXTREME_VALUE_THRESHOLD,\n",
    "        \"SPECKLE_BEYOND_STDS\": _SPECKLE_THRESHOLD,\n",
    "        \"ZSCORE_ACCEPTANCE_STDS\": _SPECKLE_NBR_Z_THRESH\n",
    "        }\n",
    "A1SpiralConfig = {\n",
    "        \"MAX_NBRS_TO_SEARCH\" : _A1_SEARCH_NBRS,\n",
    "        \"MIN_NBRS_REQUIRED\" : _A1_MIN_USED_NBRS,\n",
    "        \"MAX_NBRS_REQUIRED\" : _A1_MAX_USED_NBRS\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set AOI for calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalFillHeight = yLims[1] - yLims[0]\n",
    "totalFillWidth = xLims[1] - xLims[0]\n",
    "reqDataL = max(0, xLims[0] - _TOTAL_REQ_MARGIN)\n",
    "reqDataR = min(xLims[1] + _TOTAL_REQ_MARGIN, sourceDataWidth)\n",
    "reqDataT = max(0, yLims[0] - _TOTAL_REQ_MARGIN)\n",
    "reqDataB = min(yLims[1] + _TOTAL_REQ_MARGIN, sourceDataHeight)\n",
    "reqDataHeight = reqDataB - reqDataT\n",
    "reqDataWidth = reqDataR - reqDataL\n",
    "totalMarginL = xLims[0] - reqDataL\n",
    "totalMarginR = reqDataR - xLims[1]\n",
    "totalMarginT = yLims[0] - reqDataT\n",
    "totalMarginB = reqDataB - yLims[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate feasible slice size for A1, based on above inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBPP = 4\n",
    "outputsBPP = dataBPP*2 + 1 # the dists, output, flags\n",
    "zSize = 15 # number of years we have (at most)\n",
    "sliceSqrd = totalAvailMem / (zSize * (dataBPP+outputsBPP))\n",
    "sliceXSize = sliceSqrd / reqDataHeight \n",
    "#adjust = sliceXSize % 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate geotransform for the outputs (if non global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLeftLong = -180.0\n",
    "if xLims[0] != 0:\n",
    "    topLeftLong += xLims[0] * 0.008333333333333\n",
    "topLeftLat = 90\n",
    "if yLims[0] != 0:\n",
    "    topLeftLat -= yLims[0] * 0.008333333333333\n",
    "outputGT = (topLeftLong, 0.008333333333333, 0.0, topLeftLat, 0.0, -0.008333333333333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceXSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the slices by which A1 will be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the slice boundaries, slicing only in the x dimension \n",
    "# (we are using full data in y dimension for now, though this doesn't have to be so)\n",
    "# The slice boundaries overlap by 2* _SEARCH_RADIUS pixels (_SEARCH_RADIUS on each slice)\n",
    "# This allows the gap-filling code to run on the non-overlapping section of the slice \n",
    "# while having all the data necessary to fill a gap up to the edge of that non-overlapping \n",
    "# section\n",
    "\n",
    "nchunks = int((totalFillWidth // sliceXSize) + 1)\n",
    "# generate the \"chunk\" boundaries that will represent the data processed in one thread's job. \n",
    "chunkedges = np.linspace(xLims[0], xLims[1], nchunks+1).astype(np.int32)\n",
    "leftRealEdges = chunkedges[:-1]\n",
    "rightRealEdges = chunkedges[1:]\n",
    "left_A1_edges = np.clip((chunkedges - _A1_SEARCH_RADIUS)[:-1], 0, np.inf).astype(np.int32)\n",
    "right_A1_edges = np.clip((chunkedges + _A1_SEARCH_RADIUS)[1:], -np.inf, sourceDataWidth).astype(np.int32)\n",
    "left_Despeckle_edges = np.clip((chunkedges - _TOTAL_REQ_MARGIN)[:-1], 0, np.inf).astype(np.int32)\n",
    "right_Despeckle_edges = np.clip((chunkedges + _TOTAL_REQ_MARGIN)[1:], -np.inf, sourceDataWidth).astype(np.int32)\n",
    "\n",
    "# the left and right task boundaries are _SEARCH_RADIUS bigger than the data that will be searched \n",
    "# within them so that all pixels can have neighbours, if possible (not at global edge)\n",
    "x_offsets_overlapping = zip(left_Despeckle_edges, left_A1_edges, leftRealEdges, \n",
    "                            rightRealEdges, right_A1_edges, right_Despeckle_edges)\n",
    "\n",
    "# can we pad the top and bottom? (not if we are doing a global run as y-slicing isn't implemented)\n",
    "topA1Edge = np.clip(yLims[0]-_A1_SEARCH_RADIUS, 0, np.inf).astype(np.int32)\n",
    "bottomA1Edge = np.clip(yLims[1]+_A1_SEARCH_RADIUS, -np.inf, sourceDataHeight).astype(np.int32)\n",
    "topDespeckleEdge = np.clip(yLims[0]-_TOTAL_REQ_MARGIN, 0, np.inf).astype(np.int32)\n",
    "bottomDespeckleEdge = np.clip(yLims[1]+_TOTAL_REQ_MARGIN, -np.inf, sourceDataHeight).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of tasks that are needed to run the overall process: \n",
    "# A1 slice-by-slice and then A2 when each calendar day is complete\n",
    "a1Tasklist = list(itertools.product(\n",
    "    calendarDays,\n",
    "    x_offsets_overlapping,\n",
    "    [(topDespeckleEdge, topA1Edge, yLims[0], yLims[1], bottomA1Edge, bottomDespeckleEdge)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively restart if somebody turned the server off on you\n",
    "# restartTaskList = [t for t in a1Tasklist if int(t[0])<49]\n",
    "\n",
    "# or to run some extra data \n",
    "# extendTaskList = [t for t in a1Tasklist if 0 < int(t[0]) < 154]\n",
    "\n",
    "testTaskList = [t for t in a1Tasklist if int(t[0]) == 361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_USE_INTERMEDIATE_FILES = sliceXSize < totalFillWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_USE_INTERMEDIATE_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templateDS = gdal.Open(means_Synoptic_Path)\n",
    "bnd = templateDS.GetRasterBand(1)\n",
    "globalGT = templateDS.GetGeoTransform()\n",
    "globalProj = templateDS.GetProjection()\n",
    "templateDS = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceFileList = []\n",
    "cdDataMemMapFile = \"\"\n",
    "cdDistsMemMapFile = \"\"\n",
    "cdFlagsMemMapFile = \"\"\n",
    "currentMeans = None\n",
    "currentStds = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Run command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these cells to actually run the gapfilling, once all the cells below and above have been successfully executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startFillAt=2018000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick everything off\n",
    "global sourceFileList\n",
    "global currentMeans\n",
    "global currentMonthNumber\n",
    "\n",
    "# PICK WHAT TO DO!\n",
    "#myTaskList = oneTask\n",
    "#myTaskList = day305\n",
    "myTaskList = testTaskList\n",
    "#myTaskList = restartTaskList\n",
    "#myTaskList = day009\n",
    "#myTaskList = day193\n",
    "\n",
    "starttime = time.time()\n",
    "for sliceId in range(len(myTaskList)): \n",
    "    thisSlice = myTaskList[sliceId]\n",
    "    if sliceId == 0 or myTaskList[sliceId-1][0]!=thisSlice[0]:\n",
    "        #this is the first slice for this calendar day.\n",
    "        # find out the file names involved and load them into a filthy global\n",
    "        sourceFileList = sorted(glob.glob(strPattern.format(thisSlice[0])))\n",
    "        currentMonthNumber = dayMonths[int(thisSlice[0])]\n",
    "    despeckleAndA1Caller(thisSlice, startFillAt)\n",
    "    if sliceId == len(myTaskList) - 1 or myTaskList[sliceId+1][0] != thisSlice[0]:\n",
    "        # We assume that the tasklist is sorted by calendar day and detect when it is time to run A2 \n",
    "        # by the change in day num.\n",
    "        # this was the last slice for this calendar day. We are ready to run A2 and save out the \n",
    "        # outputs\n",
    "        a2Caller(None, startFillAt) # use e.g. a2Caller('A2008') to only run A2 on and save out the file for one year of the CD\n",
    "        \n",
    "print (\"All done in {0!s} seconds!\".format(time.time()-starttime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operational code: Load all cells below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implemented!\n",
    "def logMsg(stringMsg, terminateLine=True):\n",
    "    if terminateLine:\n",
    "        pass\n",
    "        #print stringMsg\n",
    "    else:\n",
    "        pass\n",
    "        #print stringMsg,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1 Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despeckleAndA1Caller(sliceInfo, startFillAt=None):\n",
    "    '''\n",
    "    Run Despeckle and A1 for a given slice of a calendar day (all years, same day in each).\n",
    "    \n",
    "    Calculates all the margins and other information necessary to call the despeckle and A1 cython functions for \n",
    "    a given slice or global image.\n",
    "    \n",
    "    Save outputs into uncompressed tiff or tables file (untested) that have the size of the total area\n",
    "    being processed\n",
    "    '''\n",
    "    global sourceFileList\n",
    "    global cdDataMemMapFile\n",
    "    global cdDistsMemMapFile\n",
    "    global cdFlagsMemMapFile\n",
    "   \n",
    "    global _NDV\n",
    "    global _MINMAX_CLIP_FLAG\n",
    "    \n",
    "    dayNum = sliceInfo[0]\n",
    "    xInfo = sliceInfo[1]\n",
    "    yInfo = sliceInfo[2]\n",
    "    monthNum = dayMonths[int(sliceInfo[0])]\n",
    "    \n",
    "    # Warning headfuck ensues with regard to which data has which margins!...:\n",
    "    # \n",
    "    # A1 needs a margin outside the area it is filling, if possible \n",
    "    # (i.e. if it's not a global image, which it isn't because we wouldn't have enough mem for that, and\n",
    "    # where this slice isn't at the true edge i.e. 180deg E/W or 90deg N/S)\n",
    "    # \n",
    "    # Despeckle needs a further margin outside of that so that A1 gets margin data that has itself been \n",
    "    # despeckled.\n",
    "    # A sliceInfo is structured like this:\n",
    "    # ('017',\n",
    "    # (21720, 21740, 21840, 23040, 23140, 23160),\n",
    "    # (9540, 9560, 9660, 10560, 10660, 10680))\n",
    "    \n",
    "    # Work out where the slice sits within global coordinates, prefixed with g_\n",
    "    g_sliceDespeckleL = xInfo[0]     # global coords i.e. relative to res of full raster\n",
    "    g_sliceA1L = xInfo[1]            # global coords\n",
    "    g_sliceFillL = xInfo[2]          # global coords\n",
    "    g_sliceFillR = xInfo[3]          # global coords\n",
    "    g_sliceA1R = xInfo[4]            # global coords\n",
    "    g_sliceDespeckleR = xInfo[5]     # global coords\n",
    "    \n",
    "    g_sliceDespeckleT = yInfo[0]     # global coords\n",
    "    g_sliceA1T = yInfo[1]            # global coords\n",
    "    g_sliceFillT = yInfo[2]          # global coords\n",
    "    g_sliceFillB = yInfo[3]          # global coords\n",
    "    g_sliceA1B = yInfo[4]            # global coords\n",
    "    g_sliceDespeckleB = yInfo[5]     # global coords\n",
    "    \n",
    "    # the total width that will be gapfilled and written out\n",
    "    sliceFillWidth = g_sliceFillR - g_sliceFillL\n",
    "    # the total width of data we need, equal to reqDataWidth if only 1 slice\n",
    "    sliceDespeckleWidth = g_sliceDespeckleR - g_sliceDespeckleL \n",
    "    # the total width that will be passed to a1, not actually needed\n",
    "    sliceA1Width = g_sliceA1R - g_sliceA1L \n",
    "    \n",
    "    # as above but for height, equal to reqDataHeight if only 1 slice\n",
    "    sliceFillHeight = g_sliceFillB - g_sliceFillT\n",
    "    sliceDespeckleHeight = g_sliceDespeckleB - g_sliceDespeckleT \n",
    "    sliceA1Height = g_sliceA1B - g_sliceA1T\n",
    "    assert sliceFillHeight == totalFillHeight # not actually supporting y slices for now...\n",
    "    \n",
    "    # how big a margin does despeckle actually have to work with? (as at the global edges,\n",
    "    # a margin is not possible)\n",
    "    sliceDespeckleMarginL = int(g_sliceA1L - g_sliceDespeckleL)\n",
    "    sliceDespeckleMarginR = int(g_sliceDespeckleR - g_sliceA1R)\n",
    "    sliceDespeckleMarginT = int(g_sliceA1T - g_sliceDespeckleT)\n",
    "    sliceDespeckleMarginB = int(g_sliceDespeckleB - g_sliceA1B)\n",
    "    assert ((sliceDespeckleMarginL >= 0) and (sliceDespeckleMarginR >= 0) \n",
    "            and (sliceDespeckleMarginR >= 0) and (sliceDespeckleMarginB >= 0))\n",
    "    \n",
    "    # how big a margin does A1 actually have to work with? (As at the global edges,\n",
    "    # a margin is not possible)\n",
    "    sliceA1MarginL = g_sliceFillL - g_sliceA1L\n",
    "    sliceA1MarginR = g_sliceA1R - g_sliceFillR\n",
    "    sliceA1MarginT = g_sliceFillT - g_sliceA1T\n",
    "    sliceA1MarginB = g_sliceA1B - g_sliceFillB\n",
    "    assert ((sliceA1MarginL >= 0) and (sliceA1MarginR >= 0) \n",
    "            and (sliceA1MarginR >= 0) and (sliceA1MarginB >= 0))\n",
    "    \n",
    "    # the coords within the input data that can be used, where the output (filled) \n",
    "    # data will sit\n",
    "    sliceTotalMarginT = int(sliceA1MarginT + sliceDespeckleMarginT)\n",
    "    sliceTotalMarginB = int(sliceA1MarginB + sliceDespeckleMarginB)\n",
    "    sliceTotalMarginL = int(sliceA1MarginL + sliceDespeckleMarginL)\n",
    "    sliceTotalMarginR = int(sliceA1MarginR + sliceDespeckleMarginR)\n",
    "    \n",
    "    # if we are not running the whole globe, our output files do not have \n",
    "    # global pixel coords but local ones relative to the total processing size\n",
    "    # E.g. \"Africa\" starts at global X of 18467 but this translates to X of zero in the output image\n",
    "    # Hence get the coords of this slice within the output space coordiantes, prefixed with out_\n",
    "    # - i.e. where the output slice fits into the overall output image\n",
    "    out_SliceFillL = g_sliceFillL - xLims[0] \n",
    "    out_SliceFillR = out_SliceFillL + sliceFillWidth \n",
    "    out_SliceFillT = g_sliceFillT - yLims[0]\n",
    "    out_SliceFillB = out_SliceFillT + sliceFillHeight\n",
    "    # and as we aren't yet supporting slices in the vertical dimension:\n",
    "    assert out_SliceFillT == 0\n",
    "    \n",
    "    # The the means / stds we read in have a different coordinate space again: it's the same as the \n",
    "    # output, but with the overall (total) margins added. These give the coordinates of the output image \n",
    "    # within the space of the data that is read in\n",
    "    in_SliceDataL = out_SliceFillL + totalMarginL\n",
    "    in_SliceDataR = in_SliceDataL + sliceFillWidth\n",
    "    #in_SliceDataR = in_SliceDataL +  sliceFillWidth + sliceTotalMarginR #sliceDespeckleWidth # out_SliceFillR + totalMarginL\n",
    "    in_SliceDataT = out_SliceFillT + totalMarginT\n",
    "    in_SliceDataB = in_SliceDataT + sliceFillHeight\n",
    "    #in_SliceDataB = in_SliceDataT + sliceFillHeight + sliceTotalMarginB #sliceDespeckleHeight # out_SliceFillB + totalMarginT\n",
    "    \n",
    "    # And this finally gives the coords of the slice of the mean /std in its own space\n",
    "    #processDataL = max(0,processFillL - sliceTotalMarginL)\n",
    "    #processDataR = processDataL + sliceDespeckleWidth\n",
    "    \n",
    "    print (\"************ BEGIN SLICE **************\")\n",
    "    print (\"Processing slice of global coords T:{0!s}, B:{1!s}, L:{2!s}, R:{3!s}\"\n",
    "           .format(g_sliceFillT,g_sliceFillB,g_sliceFillL,g_sliceFillR))\n",
    "    print (\"using data of calendar day {0!s}, global coords {1!s} - {2!s} ({3!s} years)\"\n",
    "           .format(dayNum, g_sliceDespeckleL, g_sliceDespeckleR, len(sourceFileList)))\n",
    "    \n",
    "    print (\"Filling output slice {0!s} - {1!s} from overall required fill width of {2!s}\"\n",
    "           .format( out_SliceFillL, out_SliceFillR, totalFillWidth))\n",
    "    print (\"Loading data from global coords {0!s} - {1!s}...\"\n",
    "           .format(g_sliceDespeckleL,g_sliceDespeckleR))\n",
    "    print (\"...into overall process coords of {0!s} - {1!s} within process data width of {2!s}\"\n",
    "           .format(in_SliceDataL, in_SliceDataR, reqDataWidth))\n",
    "     \n",
    "    if not startFillAt:\n",
    "        stackFillFromPos=0\n",
    "    else:\n",
    "        stackFillFromPos=-1\n",
    "        for y in range (len(sourceFileList)):\n",
    "            fileDate = int(os.path.basename(sourceFileList[y]).split('_')[0][1:])\n",
    "            if startFillAt and fileDate>=startFillAt:\n",
    "                stackFillFromPos = y\n",
    "                break\n",
    "        if stackFillFromPos == -1:\n",
    "            print (\"nothing to do\")\n",
    "            return\n",
    "            \n",
    " \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        stackData = np.empty(shape = (len(sourceFileList), sliceDespeckleHeight, sliceDespeckleWidth), \n",
    "                          dtype = 'Float32')\n",
    "    except MemoryError:\n",
    "        print(\"Failed to allocate sufficient memory for array of size {0!s}x{1!s}x{2!s}\".format(\n",
    "            len(sourceFileList), sliceDespeckleHeight, sliceDespeckleWidth))\n",
    "        raise\n",
    "    floatFileProps = None\n",
    "    byteFileProps = None\n",
    "    _NDV = None\n",
    " \n",
    "    # read the source files into a calendar-day stack\n",
    "    \n",
    "    for y in range (len(sourceFileList)):\n",
    "        with rasterio.open(sourceFileList[y]) as src:\n",
    "            logMsg(\".\", False)\n",
    "            stackData[y] = src.read(1,window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)),\n",
    "                                          masked=False)\n",
    "            testNDV = src.nodatavals[0]\n",
    "            if _NDV is None:\n",
    "                _NDV = testNDV\n",
    "            else:\n",
    "                assert _NDV == testNDV\n",
    "            if not floatFileProps:\n",
    "                floatFileProps = src.meta\n",
    "            if not byteFileProps:\n",
    "                byteFileProps = src.meta\n",
    "    assert stackData.flags.c_contiguous\n",
    "    print (\"Stack will be filled from position {0!s}\".format(stackFillFromPos))\n",
    "    #assert False\n",
    "    # read the means for the required \n",
    "    with rasterio.open(means_Synoptic_Path) as src:\n",
    "        sliceMeanSynoptic = src.read(1, window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)), \n",
    "                                  masked=False)\n",
    "    \n",
    "    with rasterio.open(std_Synoptic_Path) as src:\n",
    "        sliceStdSynoptic = src.read(1, window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)), \n",
    "                                 masked=False)\n",
    "\n",
    "    if means_Stack_Path is not None:\n",
    "        with rasterio.open(means_Stack_Path) as src:\n",
    "            sliceMeanMonthly = src.read(monthNum, window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)), \n",
    "                                 masked=False)\n",
    "    else:\n",
    "        sliceMeanMonthly = sliceMeanSynoptic\n",
    "        \n",
    "    if std_Stack_Path is not None:\n",
    "        with rasterio.open(std_Stack_Path) as src:\n",
    "            sliceStdMonthly = src.read(monthNum, window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)), \n",
    "                                 masked=False)\n",
    "    else:\n",
    "        sliceStdMonthly = sliceStdSynoptic\n",
    "        \n",
    "    with rasterio.open(coast_Path) as src:\n",
    "        sliceCoast = src.read(1, window=((g_sliceDespeckleT, g_sliceDespeckleB),\n",
    "                                                    (g_sliceDespeckleL, g_sliceDespeckleR)),  \n",
    "                                   masked=False)\n",
    "    \n",
    "    flags = np.zeros(shape = (len(sourceFileList),sliceDespeckleHeight,sliceDespeckleWidth), dtype=np.uint8)\n",
    "    \n",
    "    tDelta = time.time()\n",
    "    print (\"Data loaded in {0!s} seconds.\".format(\n",
    "        tDelta - start_time))\n",
    "    print (\"Despeckle margins are T:{0!s}, B:{1!s}, L:{2!s}, R:{3!s}\".format(\n",
    "        sliceDespeckleMarginT, sliceDespeckleMarginB, sliceDespeckleMarginL, sliceDespeckleMarginR))\n",
    "  \n",
    "        \n",
    "    # despeckle, initialise flags, and clip to coastline\n",
    "    #leleFlags(threadData,flags,\n",
    "    #                sliceMean,sliceStd,\n",
    "    #                sliceCoast, _NDV, _DESPECKLE_SEARCH_NBRS,\n",
    "    #                _OCEAN_FLAG, _FILL_FAILED_FLAG, _SPECKLE_FLAG, _SPECKLE_EXTREME_FLAG,\n",
    "    #                _EXTREME_VALUE_THRESHOLD, _SPECKLE_THRESHOLD,\n",
    "    #                _DATA_UPPER_LIMIT, _DATA_LOWER_LIMIT,\n",
    "    #                _DATA_CORRECTION_OFFSET,\n",
    "    #                sliceDespeckleMarginT, sliceDespeckleMarginB,\n",
    "    #                sliceDespeckleMarginL, sliceDespeckleMarginR\n",
    "    #                #,totalMarginT, totalMarginB, totalMarginL, totalMarginR\n",
    "    #                )\n",
    "    dictDataStacks = {\n",
    "                        \"Data\"            : stackData,\n",
    "                        \"Flags\"           : flags,\n",
    "                        \"Means\"           : sliceMeanMonthly,\n",
    "                        \"Stds\"            : sliceStdMonthly,\n",
    "                        \"LandMask\"        : sliceCoast,\n",
    "                        \"DistTemplate\"    : None,\n",
    "                        \"KnownUnfillable\" : None\n",
    "                        }\n",
    "    despeckleMargins = {\n",
    "                        \"TOP\"    : sliceDespeckleMarginT,\n",
    "                        \"BOTTOM\" : sliceDespeckleMarginB,\n",
    "                        \"LEFT\"   : sliceDespeckleMarginL,\n",
    "                        \"RIGHT\"  : sliceDespeckleMarginR\n",
    "                        }\n",
    "    a1Margins = {\n",
    "                        \"TOP\"    : sliceTotalMarginT,\n",
    "                        \"BOTTOM\" : sliceTotalMarginB,\n",
    "                        \"LEFT\"   : sliceTotalMarginL,\n",
    "                        \"RIGHT\"  : sliceTotalMarginR\n",
    "                        }\n",
    "    \n",
    "    deSpeckleRes = setSpeckleFlags (dictDataStacks, \n",
    "                     FlagValues, \n",
    "                     DespeckleSpiralConfig, \n",
    "                     DespeckleLimitsConfig,\n",
    "                     despeckleMargins,\n",
    "                     _NDV,\n",
    "                     _DATA_CORRECTION_OFFSET\n",
    "                     )\n",
    "    #setSpeckleFlagsOld ( stackData, flags, sliceMean, sliceStd, sliceCoast,\n",
    "    #                    _NDV, FlagValues, DespeckleSpiralConfig, DespeckleLimitsConfig,\n",
    "    #                    _DATA_CORRECTION_OFFSET, \n",
    "    #                    sliceDespeckleMarginT, sliceDespeckleMarginB, sliceDespeckleMarginL, sliceDespeckleMarginR\n",
    "    #                    )\n",
    "    dictDataStacks[\"Data\"] = deSpeckleRes[0]\n",
    "    dictDataStacks[\"Flags\"] = deSpeckleRes[1]\n",
    "    \n",
    "    print (\"Despeckled calendar day {0!s}, slice {1!s} - {2!s}, in {3!s} seconds.\".format(\n",
    "        dayNum, g_sliceDespeckleL, g_sliceDespeckleR, time.time() - tDelta))\n",
    "    tDelta = time.time()\n",
    "    \n",
    "    # Run A1! threadData and flags were modified in place by despeckle function\n",
    "    print (\"A1 Margins are T:{0!s}, B:{1!s}, L:{2!s}, R:{3!s}\".format(sliceTotalMarginT, sliceTotalMarginB, \n",
    "                                                                     sliceTotalMarginL, sliceTotalMarginR))\n",
    "    #result = fillGapsA1_cy_Additive (threadData, flags,\n",
    "    #                                    _OCEAN_FLAG, _FILL_FAILED_FLAG, _A1_FILLED_FLAG, _A1_FILL_WAS_FULL_FLAG,\n",
    "    #                                    _NDV,# _DATA_ZERO_OFFSET, _MAX_ALLOWABLE_RATIO,\n",
    "    #                                    sliceTotalMarginT, sliceTotalMarginB,\n",
    "    #                                    sliceTotalMarginL, sliceTotalMarginR)           \n",
    "    \n",
    "    result = fillGapsA1_Cy (\n",
    "                        dictDataStacks,\n",
    "                        FlagValues,\n",
    "                        A1SpiralConfig,\n",
    "                        a1Margins,\n",
    "                        _NDV,\n",
    "                        _FILL_BY_RATIO,\n",
    "                        _DATA_ZERO_OFFSET,\n",
    "                        _MAX_ALLOWABLE_RATIO,\n",
    "                        stackFillFromPos\n",
    "                        )\n",
    "    # Result contains (output,dists,new flags, waffle), \n",
    "    # all exccept waffle being of size actualHeight*actualWidth\n",
    "    del dictDataStacks\n",
    "    del stackData\n",
    "    del flags\n",
    "    \n",
    "    print (\"Ran A1 in {0!s} seconds.\".format(\n",
    "        time.time() - tDelta))\n",
    "    tDelta = time.time()                      \n",
    "    print (\"Results shape = \"+str(result[0].shape))\n",
    "    print (\"Flags shape = \"+str(result[1].shape))\n",
    "    \n",
    "    # Clip the filled values to min / max as a multiple of stds from the mean\n",
    "    if _CLIP_TO_LIMITS:\n",
    "        sliceMeanSynoptic = np.copy(sliceMeanSynoptic[sliceTotalMarginT:sliceTotalMarginT+sliceFillHeight,\n",
    "                                     sliceTotalMarginL:sliceTotalMarginL+sliceFillWidth])\n",
    "   \n",
    "        sliceStdSynoptic = np.copy(sliceStdSynoptic[sliceTotalMarginT:sliceTotalMarginT+sliceFillHeight,\n",
    "                                      sliceTotalMarginL:sliceTotalMarginL+sliceFillWidth])\n",
    "        MinMaxClip3D(result[0], result[2], sliceMeanSynoptic, sliceStdSynoptic,  \n",
    "                   _A1_FILLED_FLAG, _MINMAX_CLIP_FLAG, _FLOOR_CEILING_VALUE, _NDV, \n",
    "                   _DATA_UPPER_LIMIT, _DATA_LOWER_LIMIT)\n",
    "\n",
    "    if not _USE_INTERMEDIATE_FILES:\n",
    "        global _INTERMEDIATE_DATA\n",
    "        global _INTERMEDIATE_FLAGS\n",
    "        global _INTERMEDIATE_DISTS\n",
    "        _INTERMEDIATE_DATA = np.asarray(result[0])\n",
    "        _INTERMEDIATE_DISTS = np.asarray(result[1])\n",
    "        _INTERMEDIATE_FLAGS = np.asarray(result[2])\n",
    "        \n",
    "    elif _SAVEMETHOD == \"TIFF\":\n",
    "        outDrv = gdal.GetDriverByName('GTiff')\n",
    "        #globalGT = meansRasterStacked.GetGeoTransform()\n",
    "        #globalProj = meansRasterStacked.GetProjection()\n",
    "        #globResult = result\n",
    "        dataFNTemplate = r\"{0!s}{1!s}{2!s}{3!s}{4!s}\"\n",
    "        for y in range (len(sourceFileList)):\n",
    "            if y < stackFillFromPos:\n",
    "                continue\n",
    "            print (\"Saving intermediate file at stack position \"+str(y))\n",
    "            inFileName = os.path.basename(sourceFileList[y]).split(os.extsep)[0]\n",
    "            dataFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateData_\",inFileName,\".tif\")\n",
    "            distFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateDists_\",inFileName,\".tif\")\n",
    "            flagFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateFlags_\",inFileName,\".tif\")\n",
    "            dataRaster = None\n",
    "            distRaster = None\n",
    "            flagRaster = None\n",
    "            # no compression, no faffing with geotransforms: this is just saving a sparse array to disk\n",
    "            if os.path.exists(dataFN):\n",
    "                # this is not the first slice for this CD\n",
    "                dataRaster = gdal.Open(dataFN, gdal.GA_Update)\n",
    "            else:\n",
    "                dataRaster = outDrv.Create(dataFN,totalFillWidth,totalFillHeight,1,gdal.GDT_Float32,\n",
    "                                           [\"TILED=YES\", \"SPARSE_OK=TRUE\", \"BIGTIFF=YES\"])\n",
    "            if os.path.exists(distFN):\n",
    "                distRaster = gdal.Open(distFN, gdal.GA_Update)\n",
    "            else:\n",
    "                distRaster = outDrv.Create(distFN,totalFillWidth,totalFillHeight,1,gdal.GDT_Float32,\n",
    "                                           [\"TILED=YES\",\"SPARSE_OK=TRUE\", \"BIGTIFF=YES\"])\n",
    "\n",
    "            if os.path.exists(flagFN):\n",
    "                flagRaster = gdal.Open(flagFN,  gdal.GA_Update)\n",
    "            else:\n",
    "                flagRaster = outDrv.Create(flagFN,totalFillWidth,totalFillHeight,1,gdal.GDT_Byte,\n",
    "                                           [\"TILED=YES\",\"SPARSE_OK=TRUE\", \"BIGTIFF=YES\"])\n",
    "            bnd = dataRaster.GetRasterBand(1)\n",
    "            assert bnd is not None\n",
    "            assert dataRaster is not None\n",
    "            bnd.WriteArray(np.asarray(result[0][y]), out_SliceFillL, out_SliceFillT)\n",
    "           \n",
    "            bnd = distRaster.GetRasterBand(1)\n",
    "            assert bnd is not None\n",
    "            assert distRaster is not None\n",
    "            bnd.WriteArray(np.asarray(result[1][y]), out_SliceFillL, out_SliceFillT)\n",
    "           \n",
    "            bnd = flagRaster.GetRasterBand(1)\n",
    "            assert bnd is not None\n",
    "            assert flagRaster is not None\n",
    "            bnd.WriteArray(np.asarray(result[2][y]), out_SliceFillL, out_SliceFillT)\n",
    "            \n",
    "            #ensure flush\n",
    "            bnd = None\n",
    "            dataRaster = None\n",
    "            distRaster = None\n",
    "            flagRaster = None\n",
    "    \n",
    "    else:\n",
    "        # Save to a memory-mapped array - basic uncompressed format\n",
    "        #cdDataMemMapFile = os.path.join(tempDir, \"A1_DataMemMap_CD{0}.mmap\".format(str(dayNum)))\n",
    "        cdDataMemMapFile = os.path.join(tempDir, \"A1_FullMemMap_CD{0}.h5\".format(str(dayNum)))\n",
    "        # create the file on the first slice of each CD, else append\n",
    "        dataMode = 'w' if not os.path.exists(cdDataMemMapFile) else 'r+'\n",
    "\n",
    "        with tables.openFile(cdDataMemMapFile, dataMode) as dataTable:\n",
    "            \n",
    "            if dataMode == 'w':\n",
    "                filt = tables.Filters(complevel=1,complib='zlib',shuffle=False)\n",
    "                chunkshape = (1, 1048576/sliceFillWidth, sliceFillWidth)\n",
    "                dataMemMap = dataTable.createCArray(\n",
    "                    dataTable.root, 'IntermediateData',\n",
    "                    atom=tables.Float32Atom(),\n",
    "                    filters = filt,\n",
    "                    chunkshape = chunkshape,\n",
    "                    shape = (len(sourceFileList), totalFillHeight, totalFillWidth))\n",
    "                distMemMap = dataTable.createCArray(\n",
    "                    dataTable.root, 'IntermediateDists',\n",
    "                    atom=tables.Float32Atom(), \n",
    "                    filters = filt,\n",
    "                    chunkshape = chunkshape,\n",
    "                shape = (len(sourceFileList), totalFillHeight, totalFillWidth))\n",
    "                flagMemMap = dataTable.createCArray(\n",
    "                    dataTable.root, 'IntermediateFlags',\n",
    "                    atom=tables.UInt8Atom(), \n",
    "                    filters = filt,\n",
    "                    chunkshape = chunkshape,\n",
    "                    shape = (len(sourceFileList), totalFillHeight, totalFillWidth))\n",
    "            else:\n",
    "                dataMemMap = dataTable.root.IntermediateData\n",
    "                distMemMap = dataTable.root.IntermediateDists\n",
    "                flagMemMap = dataTable.root.IntermediateFlags\n",
    "\n",
    "            #dataMemMap = np.memmap(cdDataMemMapFile,\n",
    "             #                      dtype='Float32',\n",
    "              #                     shape=(len(sourceFileList), totalFillHeight, totalFillWidth),\n",
    "              #                     mode=dataMode)\n",
    "            \n",
    "            # we have asserted that actual height == totalheight and thus that we aren't running slices by \n",
    "            # latitude so don't bother specifying y coords\n",
    "            dataMemMap[:,:,out_SliceFillL:out_SliceFillR] = np.asarray(result[0])[:]\n",
    "            distMemMap[:,:,out_SliceFillL:out_SliceFillR] = np.asarray(result[1])[:]\n",
    "            flagMemMap[:,:,out_SliceFillL:out_SliceFillR] = np.asarray(result[2])[:]\n",
    "            dataTable.close()\n",
    "\n",
    "    #del result\n",
    "    logMsg (\"Wrote outputs in {0!s} seconds.\".format(\n",
    "        time.time() - tDelta))\n",
    "   # tDelta = time.time()\n",
    "    logMsg (\"Section {0!s} done in {1!s} seconds.\".format(\n",
    "        sliceInfo, time.time() - start_time))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2Caller(whichYearToDo = None, startFillAt = None):\n",
    "    # Load each intermediate image that has been built up by running A1 in slices\n",
    "    # Run A2 on it, if required, and save it to the output, compressed tiffs\n",
    "    global sourceFileList\n",
    "    global cdDataMemMapFile\n",
    "    global cdDistsMemMapFile\n",
    "    global cdFlagsMemMapFile\n",
    "    global currentMeans\n",
    "    global _NDV\n",
    "    global _RUN_A2\n",
    "    global stdsPreloaded\n",
    "    \n",
    "    # get means\n",
    "    src = gdal.Open(means_Synoptic_Path)\n",
    "    bnd = src.GetRasterBand(1)\n",
    "    meanImageWhole = bnd.ReadAsArray(xLims[0],yLims[0],totalFillWidth,totalFillHeight)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # the global sourceFileList contains all files of the current calendar date.\n",
    "    # we may or may not want to run a2 on all of these\n",
    "    for i in range (len(sourceFileList)):\n",
    "        if whichYearToDo is not None:\n",
    "            # whichFileToDo simply provides the ability to only run a2 on and write out one year from the \n",
    "            # stack, for quicker testing\n",
    "            if sourceFileList[i].find(whichYearToDo) == -1:\n",
    "                continue\n",
    "        elif startFillAt is not None:\n",
    "            # alternatively startFillAt provides the ability to run a2 for all files after a specific date,\n",
    "            # for updating the cube\n",
    "            fileDate = int(os.path.basename(sourceFileList[i]).split('_')[0][1:])\n",
    "            if fileDate < startFillAt:\n",
    "                continue\n",
    "            \n",
    "        if not _USE_INTERMEDIATE_FILES:\n",
    "            # A1 ran entirely in memory so just grab a reference to the filthy global\n",
    "            global _INTERMEDIATE_DATA\n",
    "            global _INTERMEDIATE_FLAGS\n",
    "            global _INTERMEDIATE_DISTS\n",
    "            fullDayData = _INTERMEDIATE_DATA[i]\n",
    "            fullDayDist = _INTERMEDIATE_DISTS[i]\n",
    "            fullDayFlags = _INTERMEDIATE_FLAGS[i]\n",
    "            \n",
    "        # load the image into memory from the mem-map or uncompressed tiff\n",
    "        elif _SAVEMETHOD==\"TIFF\":\n",
    "            dataFNTemplate = r\"{0!s}{1!s}{2!s}{3!s}{4!s}\"\n",
    "            inFileName = os.path.basename(sourceFileList[i]).split(os.extsep)[0]\n",
    "            dataFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateData_\",inFileName,\".tif\")\n",
    "            distFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateDists_\",inFileName,\".tif\")\n",
    "            flagFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateFlags_\",inFileName,\".tif\")\n",
    "            ds = gdal.Open(dataFN)\n",
    "            bnd = ds.GetRasterBand(1)\n",
    "            fullDayData = bnd.ReadAsArray()\n",
    "            ds = gdal.Open(distFN)\n",
    "            bnd = ds.GetRasterBand(1)\n",
    "            fullDayDist = bnd.ReadAsArray()\n",
    "            ds = gdal.Open(flagFN)\n",
    "            bnd = ds.GetRasterBand(1)\n",
    "            fullDayFlags = bnd.ReadAsArray()\n",
    "            ds = None\n",
    "            bnd = None\n",
    "        \n",
    "        else:\n",
    "            with tables.openFile(cdDataMemMapFile, 'r') as dataTable:\n",
    "                dataMemMapStack = dataTable.root.IntermediateData\n",
    "                distMemMapStack = dataTable.root.IntermediateDists\n",
    "                flagMemMapStack = dataTable.root.IntermediateFlags\n",
    "\n",
    "                fullDayData = np.copy(dataMemMapStack[i])\n",
    "                fullDayDist = np.copy(distMemMapStack[i])\n",
    "                fullDayFlags = np.copy(flagMemMapStack[i])\n",
    "                \n",
    "                del dataMemMapStack\n",
    "                del distMemMapStack\n",
    "                del flagMemMapStack\n",
    "   \n",
    "        if _RUN_A2:\n",
    "            # run the 8 A2 passes, which operate on the data in-place\n",
    "            A2Runner_1by1(fullDayData, fullDayFlags, fullDayDist, meanImageWhole)\n",
    "        \n",
    "        # A2_Runner_1by1 does the min-max clip\n",
    "        \n",
    "        #ds = gdal.Open(r'G:\\MOD11A2\\MOD11A2_Data\\{0}\\A2001001_LST_{0}.tif'.format(metric))\n",
    "        outDrv = gdal.GetDriverByName('GTiff')\n",
    "        \n",
    "        outFNBase = os.path.basename(sourceFileList[i]).split('.')[0]\n",
    "        outFNBaseTemplate = \"{0}_{1}.tif\"\n",
    "        outFNTemplate = os.path.join(outDir,outFNBaseTemplate)\n",
    "        print (\"Saving to\")\n",
    "        print (outFNTemplate.format(outFNBase,\"Filled_Data\"))\n",
    "        outRaster = outDrv.Create(outFNTemplate.format(outFNBase,\"Filled_Data\"),\n",
    "                                  totalFillWidth,totalFillHeight,1,gdal.GDT_Float32,\n",
    "                                  [\"COMPRESS=LZW\",\"TILED=YES\",\"BIGTIFF=YES\",\"NUM_THREADS=ALL_CPUS\"])\n",
    "        outRaster.SetGeoTransform(outputGT)\n",
    "        outRaster.SetProjection(globalProj)\n",
    "        bnd = outRaster.GetRasterBand(1)\n",
    "        bnd.SetNoDataValue(_NDV)\n",
    "        # WRITE THE DATA!\n",
    "        bnd.WriteArray(fullDayData)\n",
    "        bnd.FlushCache()\n",
    "        outRaster.FlushCache()\n",
    "        del bnd\n",
    "        outRaster = None\n",
    "\n",
    "        outRaster = outDrv.Create(outFNTemplate.format(outFNBase,\"Fill_Dists\"),\n",
    "                                  totalFillWidth,totalFillHeight,1,gdal.GDT_Float32,\n",
    "                                  [\"COMPRESS=LZW\",\"TILED=YES\",\"NUM_THREADS=ALL_CPUS\",\"BIGTIFF=YES\"])\n",
    "        outRaster.SetGeoTransform(outputGT)\n",
    "        outRaster.SetProjection(globalProj)\n",
    "        bnd = outRaster.GetRasterBand(1)\n",
    "        bnd.SetNoDataValue(_NDV)\n",
    "        # WRITE THE DATA!\n",
    "        bnd.WriteArray(fullDayDist)\n",
    "        bnd.FlushCache()\n",
    "        outRaster.FlushCache()\n",
    "        del bnd\n",
    "        outRaster = None\n",
    "        \n",
    "        outRaster = outDrv.Create(outFNTemplate.format(outFNBase,\"Fill_Flags\"),\n",
    "                                  totalFillWidth,totalFillHeight,1,gdal.GDT_Byte,\n",
    "                                  [\"COMPRESS=LZW\",\"TILED=YES\",\"NUM_THREADS=ALL_CPUS\",\"BIGTIFF=YES\"])\n",
    "        outRaster.SetGeoTransform(outputGT)\n",
    "        outRaster.SetProjection(globalProj)\n",
    "        bnd = outRaster.GetRasterBand(1)\n",
    "        bnd.SetNoDataValue(_NDV)\n",
    "        # WRITE THE DATA!\n",
    "        bnd.WriteArray(fullDayFlags)\n",
    "        bnd.FlushCache()\n",
    "        outRaster.FlushCache()\n",
    "        del bnd\n",
    "        outRaster = None\n",
    "        \n",
    "    print (time.time()-start_time)\n",
    "    \n",
    "    # Delete the intermediate files if they exist (they won't exist prior to the fill-from date\n",
    "    # if one was used by A1)\n",
    "    if _SAVEMETHOD==\"TIFF\" and _USE_INTERMEDIATE_FILES:\n",
    "        for i in range(len(sourceFileList)):\n",
    "            dataFNTemplate = r\"{0!s}{1!s}{2!s}{3!s}{4!s}\"\n",
    "            inFileName = os.path.basename(sourceFileList[i]).split(os.extsep)[0]\n",
    "            dataFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateData_\",inFileName,\".tif\")\n",
    "            distFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateDists_\",inFileName,\".tif\")\n",
    "            flagFN = dataFNTemplate.format(tempDir,os.path.sep,\"A1IntermediateFlags_\",inFileName,\".tif\")\n",
    "            if os.path.exists(dataFN):\n",
    "                os.remove(dataFN)\n",
    "            if os.path.exists(distFN):\n",
    "                os.remove(distFN)\n",
    "            if os.path.exists(flagFN):\n",
    "                os.remove(flagFN)\n",
    "                \n",
    "    elif _USE_INTERMEDIATE_FILES:\n",
    "        os.remove(cdDataMemMapFile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 multi-pass runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the 8 passes of A2 for a single image\n",
    "def A2Runner_1by1(dataImageIn, flagsImageIn, distImageIn, meanImageIn):\n",
    "    \n",
    "    global stdsPreloaded\n",
    "    assert dataImageIn.shape == flagsImageIn.shape\n",
    "    assert flagsImageIn.shape == distImageIn.shape\n",
    "       \n",
    "    assert distImageIn.shape == meanImageIn.shape\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #(dataImageIn + distImageIn + meanImageIn + dataPassStack*8 + sumDistImage + distImageLocal + ratioImageLocal)*4 + flagsImageIn\n",
    "    # = 57 bytes per pixel required, 933.1M pixels, ~50Gb RAM required. \n",
    "    # So A2 for 1km global Should _just_ be ok on a 64Gb machine! However in practice it gets memory errors because \n",
    "    # by the time it runs the intermediate data from the last slice of A1 has not been fully flushed to disk by the OS.\n",
    "    # If we have to, then we could reread data or dist images between passes, or discard the pass results and keep a single \n",
    "    # running mean from the passes, if we are using a mean rather than median selector.\n",
    "    \n",
    "    dataPassStack = np.repeat(dataImageIn[np.newaxis, :, :], 8, axis=0) # gets modified in place\n",
    "    \n",
    "    sumDistImage = np.zeros_like(dataImageIn) # gets incremented in place\n",
    "    global _NDV\n",
    "    \n",
    "    # we're not tinkering with this one, A2 will always run with 8 neighbours\n",
    "    _A2_MAX_NBRS = 8\n",
    "    \n",
    "    # The A2 cython code just runs in the same order each time on what it gets (y outer, x inner, 0-n).\n",
    "    # To get the 8 different directional passes we re-stride the inputs here, so that this causes them \n",
    "    # to be passed over in each of the directions. This means that some passes are _much_ slower than \n",
    "    # others as they aren't accessing the memory in the native contiguous order, but it saves us \n",
    "    # making a physical copy each time\n",
    "    for passNumber in range(0, 8):\n",
    "        print (\"Running A2 pass \"+str(passNumber))\n",
    "        tDelta = time.time()\n",
    "        # these will not be modified by A2, so they just get re-strided and passed in without copying\n",
    "        dataGlobalImage = None\n",
    "        flagsGlobalImage = None\n",
    "        distGlobalImage = None\n",
    "        meanGlobalImage = None\n",
    "        distImageCopy = np.copy(distImageIn)\n",
    "        # these ones get modified by A2, so they get restrided each time\n",
    "        dataPassImage = None\n",
    "        # this one gets modified by A2 and is a running total of all passes so does not get reset\n",
    "        sumDistPassImage = sumDistImage\n",
    "        outerLoop=0\n",
    "        if passNumber == 0: # pass \"a\" takes ~50s for a global image\n",
    "            dataGlobalImage = dataImageIn.T\n",
    "            flagsGlobalImage = flagsImageIn.T\n",
    "            distGlobalImage = distImageCopy.T\n",
    "            meanGlobalImage = meanImageIn.T\n",
    "            sumDistPassImage = sumDistImage.T\n",
    "            dataPassImage = dataPassStack[passNumber].T\n",
    "            outerLoop=1\n",
    "        elif passNumber == 1: # pass \"b\" ~50s\n",
    "            dataGlobalImage = dataImageIn.T[:,::-1]\n",
    "            flagsGlobalImage = flagsImageIn.T[:,::-1]\n",
    "            distGlobalImage = distImageCopy.T[:,::-1]\n",
    "            meanGlobalImage = meanImageIn.T[:,::-1]\n",
    "            sumDistPassImage = sumDistImage.T[:,::-1]\n",
    "            dataPassImage = dataPassStack[passNumber].T[:,::-1]\n",
    "            outerLoop=1\n",
    "        elif passNumber == 2: # pass \"c\" ~50s\n",
    "            dataGlobalImage = dataImageIn.T[::-1,:]\n",
    "            flagsGlobalImage = flagsImageIn.T[::-1,:]\n",
    "            distGlobalImage = distImageCopy.T[::-1,:]\n",
    "            meanGlobalImage = meanImageIn.T[::-1,:]\n",
    "            sumDistPassImage = sumDistImage.T[::-1,:]\n",
    "            dataPassImage = dataPassStack[passNumber].T[::-1,:]\n",
    "            outerLoop=1\n",
    "        elif passNumber == 3: # pass \"d\" ~50s\n",
    "            dataGlobalImage = dataImageIn.T[::-1,::-1]\n",
    "            flagsGlobalImage = flagsImageIn.T[::-1,::-1]\n",
    "            distGlobalImage = distImageCopy.T[::-1,::-1]\n",
    "            meanGlobalImage = meanImageIn.T[::-1,::-1]\n",
    "            sumDistPassImage = sumDistImage.T[::-1,::-1]\n",
    "            dataPassImage = dataPassStack[passNumber].T[::-1,::-1]\n",
    "            outerLoop=1\n",
    "        elif passNumber == 4: # pass \"e\" - this one is the native c-ordering, ~7s!\n",
    "            dataGlobalImage = dataImageIn\n",
    "            flagsGlobalImage = flagsImageIn\n",
    "            distGlobalImage = distImageCopy\n",
    "            meanGlobalImage = meanImageIn\n",
    "            sumDistPassImage = sumDistImage\n",
    "            dataPassImage = dataPassStack[passNumber]\n",
    "            \n",
    "        elif passNumber == 5: # pass \"f\" ~9s\n",
    "            dataGlobalImage = dataImageIn[:,::-1]\n",
    "            flagsGlobalImage = flagsImageIn[:,::-1]\n",
    "            distGlobalImage = distImageCopy[:,::-1]\n",
    "            meanGlobalImage = meanImageIn[:,::-1]\n",
    "            sumDistPassImage = sumDistImage[:,::-1]\n",
    "            dataPassImage = dataPassStack[passNumber][:,::-1]\n",
    "            \n",
    "        elif passNumber == 6: # pass \"g\" ~8s\n",
    "            dataGlobalImage = dataImageIn[::-1,:]\n",
    "            flagsGlobalImage = flagsImageIn[::-1,:]\n",
    "            distGlobalImage = distImageCopy[::-1,:]\n",
    "            meanGlobalImage = meanImageIn[::-1,:]\n",
    "            sumDistPassImage = sumDistImage[::-1,:]\n",
    "            dataPassImage = dataPassStack[passNumber][::-1,:]\n",
    "            \n",
    "        else: #elif passNumber == 7: # pass \"h\" ~8s\n",
    "            dataGlobalImage = dataImageIn[::-1,::-1]\n",
    "            flagsGlobalImage = flagsImageIn[::-1,::-1]\n",
    "            distGlobalImage = distImageCopy[::-1,::-1]\n",
    "            meanGlobalImage = meanImageIn[::-1,::-1]\n",
    "            sumDistPassImage = sumDistImage[::-1,::-1]\n",
    "            dataPassImage = dataPassStack[passNumber][::-1,::-1]\n",
    "        \n",
    "        #print \"a2 pass \"+str(passNumber)+\"...\",\n",
    "        \n",
    "        fillGapsA2_Cy (\n",
    "             { \n",
    "                \"Data\":      dataGlobalImage,\n",
    "                \"Flags\":     flagsGlobalImage,\n",
    "                \"Distances\": distGlobalImage,\n",
    "                \"Means\":     meanGlobalImage,\n",
    "                \"SumDist\":   sumDistPassImage,\n",
    "                \"Output\":    dataPassImage\n",
    "             },\n",
    "             FlagValues,\n",
    "             _NDV,\n",
    "             _A2_MAX_NBRS,\n",
    "             _FILL_BY_RATIO,\n",
    "             _DATA_ZERO_OFFSET,\n",
    "             _MAX_ALLOWABLE_RATIO\n",
    "         )\n",
    "\n",
    "        #print \"done in \"+str(time.time()-tDelta)+\" seconds\"\n",
    "    \n",
    "    tDelta = time.time()\n",
    "    # process numpyisms on the A2 data stack tile-by-tile because it makes temp arrays,\n",
    "    # and we probably can't afford that memory\n",
    "    xCorners = np.linspace(0,dataImageIn.shape[1],6).astype(np.int32)\n",
    "    yCorners = np.linspace(0,dataImageIn.shape[0],6).astype(np.int32)\n",
    "    medianVals = np.empty_like(dataImageIn) # or mean, depending on choice\n",
    "    countVals = np.empty(shape=dataImageIn.shape,dtype='byte')\n",
    "    for x in range(len(xCorners)-1):\n",
    "        for y in range(len(yCorners)-1):\n",
    "            x0 = xCorners[x]\n",
    "            x1 = xCorners[x+1]\n",
    "            y0 = yCorners[y]\n",
    "            y1 = yCorners[y+1]\n",
    "            dataPassSlice = dataPassStack[:,y0:y1,x0:x1]\n",
    "            countSlice = countVals[y0:y1,x0:x1]\n",
    "            np.sum(dataPassSlice != _NDV, axis=0, out=countSlice)\n",
    "            # exclude no-data from mean / median calculation by using nan-aware functions\n",
    "            dataPassSlice[dataPassSlice == _NDV] = np.nan\n",
    "            medianValsSlice = medianVals[y0:y1,x0:x1]\n",
    "            if _A2_PASS_SELECTOR == \"MEAN\":\n",
    "                medianValsSlice[:] = bn.nanmean(dataPassSlice, axis=0)\n",
    "            else:\n",
    "                medianValsSlice[:] = bn.nanmedian(dataPassSlice, axis=0)\n",
    "            \n",
    "            \n",
    "    dataPassStack = None\n",
    "    del dataPassStack # and breathe\n",
    "    gc.collect()\n",
    "    \n",
    "    # A2 doesn't modify the flags image, so anywhere that the fill had failed is where it was \n",
    "    # needed\n",
    "    a2FillWasNeeded = np.bitwise_and(flagsImageIn,_FILL_FAILED_FLAG)==_FILL_FAILED_FLAG\n",
    "    a2FillWasNeededCount = np.count_nonzero(a2FillWasNeeded)\n",
    "    # we have a value for anywhere that the output isn't _NDV (now np.nan)\n",
    "    a2FilledLocs = np.logical_and(a2FillWasNeeded,~np.isnan(medianVals))\n",
    "    a2FilledCount = np.count_nonzero(a2FilledLocs)\n",
    "    # so copy those a2 results into the \"output\" data image\n",
    "    dataImageIn[a2FilledLocs] = medianVals[a2FilledLocs]\n",
    "    flagsImageIn[a2FilledLocs] -= _FILL_FAILED_FLAG # should use bitwise ^ really\n",
    "    flagsImageIn[a2FilledLocs] += _A2_SUCCESS_FLAG  # should use bitwise or\n",
    "    # set distance metric to be the total A2 distance divided by the number of A2 passes it was from\n",
    "    distImageIn[a2FilledLocs] = sumDistImage[a2FilledLocs] / countVals[a2FilledLocs]\n",
    "    \n",
    "    if _CLIP_TO_LIMITS:\n",
    "        # if we don't have memory to store global stds, we could load them again here \n",
    "        # as they are only needed at this point so we don't need them in mem at same \n",
    "        # time as the A2 stack\n",
    "        #with rasterio.open(std_Synoptic_Path) as src:\n",
    "        if stdsPreloaded is None:\n",
    "            src = gdal.Open(std_Synoptic_Path)\n",
    "            bnd = src.GetRasterBand(1)\n",
    "            stds = bnd.ReadAsArray(xLims[0],yLims[0],totalFillWidth,totalFillHeight)\n",
    "            bnd = None\n",
    "            src = None\n",
    "                #stds = src.read_band(1, window=((totalMarginT,totalMarginT+totalFillHeight),\n",
    "                #                                        (totalMarginL,totalMarginL+totalFillWidth)), \n",
    "                #                     masked=False)\n",
    "        else:\n",
    "            stds = stdsPreloaded\n",
    "        MinMaxClip(dataImageIn, flagsImageIn, meanImageIn, stds,\n",
    "                   _A2_SUCCESS_FLAG, _MINMAX_CLIP_FLAG, _FLOOR_CEILING_VALUE, _NDV,\n",
    "                   _DATA_UPPER_LIMIT, _DATA_LOWER_LIMIT)\n",
    "    #todo - clip to min/max?\n",
    "    #print \"Flattening done in \"+str(time.time()-tDelta)+\" seconds\"\n",
    "    print (\"A2 done, filled {0} locations of {1} required in {2} seconds using {3} of 8 passes\".format(\n",
    "                                                    a2FilledCount,a2FillWasNeededCount,\n",
    "                                                    str(time.time()-start_time), _A2_PASS_SELECTOR))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython functions below this point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions that involve iterating over the data and which cannot easily be vectorised have been rewritten in Cython with all optimisations used. For example, bounds checking turned off, all arrays structured and iterated over (excepy in A2) in native C order, all variables typed, etc. (They were first checked with bounds check etc turned on!). \n",
    "\n",
    "Most of the centre loops have been parallelised using OpenMP via Cython's tools and set to an appopriate number of threads for the 64-core machines that they were run on.\n",
    "\n",
    "Thanks to the simplicity of Cython for writing python code that is then translated to optimised C, the algorithms are visually very little changed in comparison to the versions published in the original paper, which were also simple looping functions. \n",
    "\n",
    "Of note is that a vectorised version of A1 was produced and tested (based on numpy) but it was far, far slower than these Cython versions even on one thread.\n",
    "\n",
    "Using IPython notebook (once properly configured) the Cython functions can be compiled and loaded simply by running the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despeckle and create flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=/openmp --link-args=/openmp --force\n",
    "# %%cython  --compile-args=-fopenmp --link-args=-fopenmp --force\n",
    "# the syntax of the arguments above is for Linux - it is different for windows and if this isn't set \n",
    "# then the openmp support won't actually be switched on (though no error will be seen) and the code \n",
    "# will only run on 1 thread\n",
    "# On windows use \n",
    "#%% cython --compile-args=/openmp --link-args=/openmp --force\n",
    "import numpy as np\n",
    "from libc.math cimport fabs, sqrt\n",
    "cimport cython\n",
    "cimport openmp\n",
    "from cython.parallel import prange, parallel\n",
    "@cython.cdivision(True)\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cpdef setSpeckleFlags (\n",
    "                   dict DataStacks,\n",
    "                   dict FlagValues,\n",
    "                   dict SpiralSearchConfig,\n",
    "                   dict Limits,\n",
    "                   dict Margins,\n",
    "                   float _NDV,\n",
    "                   float _CORRECTION_OFFSET = 0,\n",
    "                   ):\n",
    "    \n",
    "    '''\n",
    "    Cython implementation of the \"despeckle\" algorithm; also applies land-sea mask and any systematic correction.\n",
    "    \n",
    "    Runs the \"despeckle\" algorithm, replacing cells that differ by more than \n",
    "    N1 standard deviations from the local mean with nodata, and replacing cells that differ \n",
    "    by more than N2 standard deviations (where N1 > N2) from the local mean with nodata iif \n",
    "    their z nearest neighbours don't on average also do so.\n",
    "    '''\n",
    "   \n",
    "    # data, means, stds should have margins allowing the neighbour search to run\n",
    "    # flags and landMask should be of the shape that is data shape - margins\n",
    "    # data and flags will be modified in place ready for passing to A1 routine\n",
    "    cdef:\n",
    "        # 2d arrays for the upper and lower validity / possible speckle limits\n",
    "        float [:,::1] validLower, validUpper, dodgyLower, dodgyUpper\n",
    "        # 2d array of the zscores for the valid cells in the current day\n",
    "        float[:,::1] zScoresDay\n",
    "        int[:,::1] nbrIntCoords\n",
    "        # loop vars etc\n",
    "        Py_ssize_t zShape, xShapeTotal, yShapeTotal\n",
    "        Py_ssize_t xShapeToDespeckle, yShapeToDespeckle\n",
    "        # mnemonic, yT = total (outer coords), yD is shifted into the output depeckled coords\n",
    "        Py_ssize_t yT, xT_prv, xD_prv, yD_prv, xDNbr_prv, yDNbr_prv\n",
    "        float value_prv\n",
    "        long long speckleCount_Glob, extremeCount_Glob, goodCount_Glob, oceanCount_Glob, clearedSpeckleCount_Glob\n",
    "        Py_ssize_t nbrIndex_prv\n",
    "        float nbrZscore_prv, zscoreDiff_prv\n",
    "        int nbrZscoreCount_prv\n",
    "        float nbrZscoreTotal_prv\n",
    "       \n",
    "        float _BLANK_ZSCORE\n",
    "    \n",
    "    # unpack input dictionaries to typed variables\n",
    "    cdef:\n",
    "        # thresholds / consts\n",
    "        float hardLowerLimit = Limits[\"HARD_LOWER_LIMIT\"]\n",
    "        float hardUpperLimit = Limits[\"HARD_UPPER_LIMIT\"]\n",
    "        float _SPECKLE_ZSCORE_THRESHOLD = Limits[\"ZSCORE_ACCEPTANCE_STDS\"]\n",
    "        float stDevValidityThreshold = Limits[\"INVALID_BEYOND_STDS\"]\n",
    "        float speckleDevThreshold = Limits[\"SPECKLE_BEYOND_STDS\"]\n",
    "  \n",
    "        int _SPECKLE_NBR_MIN_THRESHOLD = SpiralSearchConfig[\"MIN_NBRS_REQUIRED\"]\n",
    "        int _SPECKLE_NBR_MAX_THRESHOLD = SpiralSearchConfig[\"MAX_NBRS_REQUIRED\"]\n",
    "        int _MAX_NEIGHBOURS_TO_CHECK = SpiralSearchConfig[\"MAX_NBRS_TO_SEARCH\"]\n",
    "        \n",
    "        short _OCEAN_FLAG = FlagValues[\"OCEAN\"]\n",
    "        short _NEVERDATA_FLAG = FlagValues[\"FAILURE\"]\n",
    "        short _EXTREME_FLAG = FlagValues[\"EXTREME\"]\n",
    "        short _SPECKLE_FLAG = FlagValues[\"SPECKLE\"]\n",
    "        \n",
    "        int marginT = Margins[\"TOP\"]\n",
    "        int marginB = Margins[\"BOTTOM\"]\n",
    "        int marginL = Margins[\"LEFT\"] \n",
    "        int marginR = Margins[\"RIGHT\"]\n",
    "        \n",
    "        float[:,:,::1] data = DataStacks[\"Data\"]\n",
    "        unsigned char[:,:,::1] flags = DataStacks[\"Flags\"]\n",
    "        float[:,::1] means = DataStacks[\"Means\"]\n",
    "        float[:,::1] stds = DataStacks[\"Stds\"]\n",
    "        char[:,::1] landMask = DataStacks[\"LandMask\"]\n",
    "        \n",
    "        float[:,:,::1] outputData\n",
    "        \n",
    "    zShape = data.shape[0]\n",
    "    yShapeTotal = data.shape[1]\n",
    "    xShapeTotal = data.shape[2]\n",
    "    # we will only iterate thru cells that are not in the margins\n",
    "    yShapeToDespeckle = data.shape[1] - (marginT + marginB)\n",
    "    xShapeToDespeckle = data.shape[2] - (marginL + marginR)\n",
    "    # and we will only set flags within the cells that A1 cares about\n",
    "    #yShapeActual = data.shape[1] - (marginTTot + marginBTot)\n",
    "    #xShapeActual = data.shape[2] - (marginLTot + marginRTot)\n",
    "    \n",
    "    assert means.shape[0] == stds.shape[0] == yShapeTotal\n",
    "    assert means.shape[1] == stds.shape[1] == xShapeTotal\n",
    "    assert landMask.shape[0] == yShapeTotal\n",
    "    assert landMask.shape[1] == xShapeTotal\n",
    " \n",
    "    assert flags.shape[1] == yShapeTotal\n",
    "    assert flags.shape[2] == xShapeTotal\n",
    "    \n",
    "    validLower = np.empty(shape=(yShapeTotal, xShapeTotal), dtype='float32', order='c')\n",
    "    validUpper = np.empty(shape=(yShapeTotal, xShapeTotal), dtype='float32', order='c')\n",
    "    dodgyLower = np.empty(shape=(yShapeTotal, xShapeTotal), dtype='float32', order='c')\n",
    "    dodgyUpper = np.empty(shape=(yShapeTotal, xShapeTotal), dtype='float32', order='c')\n",
    "    zScoresDay = np.empty(shape=(yShapeTotal, xShapeTotal), dtype='float32', order='c')\n",
    "    \n",
    "    _BLANK_ZSCORE = -999.0\n",
    "    speckleCount_Glob = 0\n",
    "    extremeCount_Glob = 0\n",
    "    goodCount_Glob = 0\n",
    "    oceanCount_Glob = 0\n",
    "    clearedSpeckleCount_Glob = 0\n",
    "    \n",
    "    print (\"Despeckle: Rejecting data beyond {0!s}s.d. of mean. Nbr search on data beyond {1!s} s.d. of mean.\".\n",
    "           format(stDevValidityThreshold, speckleDevThreshold))\n",
    "    print (\"Nbr searching for {0!s} - {1!s} nbrs within {2!s} spiral steps for z-score tolerance of {3!s}\".\n",
    "           format( _SPECKLE_NBR_MIN_THRESHOLD, _SPECKLE_NBR_MAX_THRESHOLD, _MAX_NEIGHBOURS_TO_CHECK, \n",
    "                  _SPECKLE_ZSCORE_THRESHOLD))\n",
    "    \n",
    "    # Generate the neighbour spiral search table out to \"a bit\" further than needed\n",
    "    _SEARCH_RADIUS =  <int> ((sqrt(_MAX_NEIGHBOURS_TO_CHECK / 3.14)) + 5)\n",
    "    diam = _SEARCH_RADIUS * 2 + 1\n",
    "    print (\"Despeckle diam = \" + str(diam))\n",
    "    inds = np.indices([diam, diam]) - _SEARCH_RADIUS\n",
    "    distTmp = np.sqrt((inds ** 2).sum(0))\n",
    "    npTmpTable = ((inds.T).reshape(diam ** 2, 2))\n",
    "    npTmpTable = np.append(npTmpTable, distTmp.ravel()[:, None],axis=1)\n",
    "    \n",
    "    # sort the table by distance then x then y (the arguments are last-sort-first)\n",
    "    order = np.lexsort((npTmpTable[:, 1],npTmpTable[:, 0],npTmpTable[:, 2]))\n",
    "    npTmpTable = np.take(npTmpTable, order, axis=0)\n",
    "    \n",
    "    # transfer to a C-side object transposed to have three rows and many columns and in \n",
    "    # C-contiguous layout, so that cython can access individual nbr coord sets more quickly\n",
    "    nbrTable = np.copy((npTmpTable[npTmpTable[:,2] <= _SEARCH_RADIUS]).T, order='c')\n",
    "    # cast the columns that will be used as array indices to int type once here, rather \n",
    "    # than casting repeatedly inside the inner loop\n",
    "    nbrIntCoords = np.asarray(nbrTable[0:2,:]).astype(np.int32)\n",
    "    \n",
    "    # We can't modify the input in this algorithm as we have parallelised it\n",
    "    # and we look at adjacent results in setting the cell's value - we only want to use\n",
    "    # original data for this check, so we have to keep a pristine copy\n",
    "    outputData = np.empty_like(data, order='c')\n",
    "    outputData[:] = _NDV\n",
    "    \n",
    "    # We need to run speckle check IN the margins so the data in those margins can be eliminated if necessary\n",
    "    # and not used in checking other values not in the margins\n",
    "    # However the speckle search radius is smaller than the A1 radius\n",
    "    # So we give speckle check a smaller margin so it can be happy but everything that A1 needs \n",
    "    # has been despeckled\n",
    "    \n",
    "    # Apply any correction offset (to correct messed-up celsius-kelvin conversion!!)\n",
    "    if _CORRECTION_OFFSET != 0:\n",
    "        for z in range (zShape):\n",
    "            with nogil, parallel(num_threads=20):\n",
    "                for yT in prange (yShapeTotal, schedule='static', chunksize=200):\n",
    "                    xT_prv = -1\n",
    "                    for xT_prv in range (xShapeTotal):\n",
    "                        if data[z, yT, xT_prv] != _NDV:\n",
    "                            data[z, yT, xT_prv] = data[z, yT, xT_prv] + _CORRECTION_OFFSET\n",
    "                \n",
    "    # Precalculate the thresholds. These are the same for every day in the stack so spend \n",
    "    # some memory to avoid doing it 15 times (this is the only economy in calling this routine\n",
    "    # with a 3d array, so if memory is an issue just modify to run on a 2d array one day at a time)\n",
    "    # run for total shape i.e. including speckle margins\n",
    "    with nogil, parallel(num_threads=20):\n",
    "        for yT in prange (yShapeTotal, schedule='static', chunksize=200):\n",
    "            xT_prv = -1\n",
    "            for xT_prv in range (xShapeTotal):\n",
    "                if landMask[yT,xT_prv] == 0:\n",
    "                    continue\n",
    "                if means[yT, xT_prv] == _NDV:\n",
    "                    continue\n",
    "                if _CORRECTION_OFFSET != 0:\n",
    "                    means[yT, xT_prv] = means[yT, xT_prv] + _CORRECTION_OFFSET\n",
    "                validLower[yT,xT_prv] = means[yT,xT_prv] - stds[yT,xT_prv] * stDevValidityThreshold\n",
    "                validUpper[yT,xT_prv] = means[yT,xT_prv] + stds[yT,xT_prv] * stDevValidityThreshold\n",
    "                dodgyLower[yT,xT_prv] = means[yT,xT_prv] - stds[yT,xT_prv] * speckleDevThreshold\n",
    "                dodgyUpper[yT,xT_prv] = means[yT,xT_prv] + stds[yT,xT_prv] * speckleDevThreshold\n",
    "\n",
    "    # now identify cells which _may_ be speckles by comparison to the limits just calculated\n",
    "    # also apply land-sea mask and calculate z scores in this pass\n",
    "    # Run on full dataset incl margins\n",
    "    for z in range (zShape): \n",
    "        #re initialise zscoresday\n",
    "        zScoresDay[:] = _BLANK_ZSCORE\n",
    "        with nogil,parallel(num_threads=10):\n",
    "            # do for everything, including speckle margins\n",
    "            for yT in prange (yShapeTotal, schedule='static', chunksize=200):\n",
    "                # assign to the inner loop's variables so that Cython will make them thread-private\n",
    "                xT_prv = -1\n",
    "                value_prv = _NDV\n",
    "                nbrIndex_prv = -1\n",
    "                nbrZscoreCount_prv = 0\n",
    "                nbrZscoreTotal_prv = 0.0\n",
    "                nbrZscore_prv = 0.0\n",
    "                zscoreDiff_prv = 0.0\n",
    "                \n",
    "                for xT_prv in range(xShapeTotal):\n",
    "                    if landMask[yT, xT_prv] == 0:\n",
    "                        # do not do anything where it is not in the land\n",
    "                        # just set the flags image to record this\n",
    "                        flags[z, yT, xT_prv] = flags[z, yT, xT_prv] | _OCEAN_FLAG\n",
    "                        \n",
    "                        # Do not copy across any data existing in the sea. It's likely to be squiffy\n",
    "                        # (EVI in particular) and if we leave it, the fill process inland could use these \n",
    "                        # dodgy values\n",
    "                        # (Output data is already NDV from initialisation)\n",
    "                        \n",
    "                        # Tracking\n",
    "                        oceanCount_Glob += 1\n",
    "                        continue\n",
    "                        \n",
    "                    # if no mean then it means we never have data in the stack and will not be able to \n",
    "                    # produce a fill\n",
    "                    if means[yT, xT_prv] == _NDV:\n",
    "                        flags[z, yT, xT_prv] = flags[z, yT, xT_prv] | _NEVERDATA_FLAG\n",
    "                        continue\n",
    "                        \n",
    "                    value_prv = data[z, yT, xT_prv]\n",
    "                    # is the value outside extreme thresholds? delete if so\n",
    "                    if (value_prv < hardLowerLimit or value_prv > hardUpperLimit or \n",
    "                        value_prv < validLower[yT, xT_prv] or value_prv > validUpper[yT, xT_prv] \n",
    "                        or value_prv == _NDV):\n",
    "                        if value_prv != _NDV:\n",
    "                            flags[z, yT, xT_prv] = flags[z, yT, xT_prv] | _EXTREME_FLAG\n",
    "                            extremeCount_Glob += 1\n",
    "                        # no flag to set if it IS nodata because the fact it is nodata is all the flag\n",
    "                        # we need...\n",
    "                        continue\n",
    "                        \n",
    "                    # implicit else\n",
    "                    # pre-calculate the z-scores for all cells that are not definitely invalid\n",
    "                    zScoresDay[yT, xT_prv] = (value_prv - means[yT, xT_prv]) / stds[yT, xT_prv]\n",
    "                    outputData [z, yT, xT_prv] = value_prv\n",
    "                    # see if the location is a possible speckle\n",
    "                    if value_prv >= dodgyLower[yT, xT_prv] and value_prv <= dodgyUpper[yT, xT_prv]:\n",
    "                        # it's within the local limits, so it counts as good data, so pass through\n",
    "                        goodCount_Glob += 1\n",
    "                    else:\n",
    "                        # it's outside the speckle thresholds, but within the validity thresholds\n",
    "                        # so set the speckle flag (we may remove it again later after the nbr check)\n",
    "                        flags[z, yT, xT_prv] = flags[z, yT, xT_prv] | _SPECKLE_FLAG \n",
    "        \n",
    "        # now go through again, the z-scores are all populated wherever possible so no need \n",
    "        # to re-calculate every time a nbr is checked as in orginal implementation\n",
    "        # do not run for speckle margins - use inner shape - but this will still include the A1 margins\n",
    "        # if everything's been set up right!\n",
    "        with nogil,parallel(num_threads=10):\n",
    "            for yT in prange (yShapeToDespeckle, schedule='dynamic', chunksize=200):\n",
    "                \n",
    "                xT_prv = -1\n",
    "                xD_prv = -1\n",
    "                yD_prv = yT + marginT\n",
    "                xDNbr_prv = -1\n",
    "                yDNbr_prv = -1\n",
    "                \n",
    "                nbrIndex_prv = -1\n",
    "                nbrZscoreCount_prv = 0\n",
    "                nbrZscoreTotal_prv = 0.0\n",
    "                nbrZscore_prv = 0.0\n",
    "                zscoreDiff_prv = 0.0\n",
    "                \n",
    "                for xT_prv in range(xShapeToDespeckle):\n",
    "                    xD_prv = xT_prv + marginL\n",
    "                    nbrIndex_prv = -1\n",
    "                    nbrZscoreCount_prv = 0\n",
    "                    nbrZscoreTotal_prv = 0.0\n",
    "                    nbrZscore_prv = 0.0\n",
    "                    zscoreDiff_prv = 0.0\n",
    "                    if (flags[z, yD_prv, xD_prv] & _SPECKLE_FLAG) != _SPECKLE_FLAG:\n",
    "                        continue\n",
    "                    \n",
    "                    # else we are on a possible speckle\n",
    "                    nbrZscoreCount_prv = 0\n",
    "                    nbrZscoreTotal_prv = 0.0\n",
    "                    # spiral search of the nbrs to see if they too are far from \n",
    "                    # the mean in which case this isn't a speckle after all\n",
    "                    for nbrIndex_prv in range(1, _MAX_NEIGHBOURS_TO_CHECK + 1):\n",
    "                        # This loop may run 100s or 1000s of times for every potential\n",
    "                        # speckle cell, i.e. potentially trillions of times.\n",
    "                        # So it's really important to get this loop fast.\n",
    "                        if nbrZscoreCount_prv == _SPECKLE_NBR_MAX_THRESHOLD:\n",
    "                            # we've found sufficient neighbours so stop looking\n",
    "                            break\n",
    "                        # use int-type coords array to avoid cast op in tight loop\n",
    "                        xDNbr_prv = xD_prv + nbrIntCoords[0, nbrIndex_prv]\n",
    "                        yDNbr_prv = yD_prv + nbrIntCoords[1, nbrIndex_prv]\n",
    "\n",
    "                        # is the requested neighbour cell within data bounds and with data?\n",
    "                        if (xDNbr_prv >= 0 and xDNbr_prv < xShapeTotal and \n",
    "                                yDNbr_prv >= 0 and yDNbr_prv < yShapeTotal and\n",
    "                                zScoresDay[yDNbr_prv, xDNbr_prv] != _BLANK_ZSCORE):\n",
    "                            nbrZscoreCount_prv = nbrZscoreCount_prv + 1 # tracking\n",
    "                            nbrZscoreTotal_prv = nbrZscoreTotal_prv + zScoresDay[yDNbr_prv, xDNbr_prv]\n",
    "                    \n",
    "                    # did we find enough neighbours? \n",
    "                    if nbrZscoreCount_prv >= _SPECKLE_NBR_MIN_THRESHOLD:\n",
    "                        # were the neighbours similar to this cell in terms of distance from mean?\n",
    "                        nbrZscore_prv = nbrZscoreTotal_prv / nbrZscoreCount_prv\n",
    "                        zscoreDiff_prv = fabs(nbrZscore_prv - zScoresDay[yD_prv, xD_prv])\n",
    "                        if zscoreDiff_prv < _SPECKLE_ZSCORE_THRESHOLD:\n",
    "                            # this pixels neighbours are also far from the mean, it's probably\n",
    "                            # a volcano or a country that caught fire or something, so we'll believe it,\n",
    "                            # clear the flag, and carry through the original value\n",
    "                            flags[z, yD_prv, xD_prv] = flags[z, yD_prv, xD_prv] ^ _SPECKLE_FLAG\n",
    "                            clearedSpeckleCount_Glob += 1\n",
    "                            continue\n",
    "                    # insufficient neighbours or the neighbours are less extreme.\n",
    "                    # so this is a speckle; delete the data\n",
    "                    speckleCount_Glob += 1\n",
    "                    outputData[z, yD_prv, xD_prv] = _NDV\n",
    "                    \n",
    "    print (\"Speckle count:    \" + str(speckleCount_Glob))\n",
    "    print (\"Extreme count:    \" + str(extremeCount_Glob))\n",
    "    print (\"Good count:       \" + str(goodCount_Glob))\n",
    "    print (\"Cleared Speckle:  \" + str(clearedSpeckleCount_Glob) )\n",
    "    print (\"Ocean count:      \" + str(oceanCount_Glob))\n",
    "    return (outputData, flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=/openmp --link-args=/openmp --force\n",
    "#%%cython  --compile-args=-fopenmp --link-args=-fopenmp --force\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "from cython.parallel import prange, parallel\n",
    "import numpy as np\n",
    "from libc.stdlib cimport abs\n",
    "from libc.math cimport sqrt\n",
    "cdef int _TRIM_MIN_MAX = 1\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cdef int[::1] alternates_cy(int lim):\n",
    "    ''' yields 1, -1, 2, -2, etc up to lim '''\n",
    "    cdef int x\n",
    "    x = 1\n",
    "    cdef int[::1] arr = np.empty(shape=(lim*2),dtype=np.int32,order='c')\n",
    "    for x in range(0,lim): \n",
    "        arr[2*x] = x+1\n",
    "        arr[2*x+1] = -(x+1)\n",
    "        x += 1\n",
    "    return arr\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "# pass in the whole data that should be checked, and optionally the margins that are provided\n",
    "# - we will only fill gaps within the portion of the data that isn't excluded in the margin,\n",
    "# but we will use the margin data to provide neighbour data for those pixels\n",
    "# So for a tiled run data can (should) be provided with a padding of _SEARCH_RADIUS and the \n",
    "# margin set to this value also. That way the algorithm can get values for \"edge pixels\" of its\n",
    "# input data\n",
    "\n",
    "cpdef fillGapsA1_Cy(dict DataStacks, # has items Data, Flags, DistTemplate (optional), KnownUnfillable (optional)\n",
    "                    dict FlagValues,\n",
    "                    dict SpiralSearchConfig,\n",
    "                    dict Margins, # has items Top, Bottom, Left, Right\n",
    "                    float _NDV, \n",
    "                    char FillByRatios = 0,\n",
    "                    float RatioAbsZeroPoint = 0,\n",
    "                    float RatioLimit = 1, \n",
    "                    char RunFillFromPos = 0\n",
    "                ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Optimised, multithreaded Cython (C) implementation of the A1 gapfilling algorithm.\n",
    "    \n",
    "    fillGapsA1_Cy(dict DataStacks, dict FlagValues, dict SpiralSearchConfig, dict Margins, \n",
    "                float _NDV, char FillByRatios=0, float RatioAbsZeroPoint=0, float RatioLimit=1,\n",
    "                char RunFillFromPos = 0) \n",
    "                    -> \n",
    "                    (float[:,:,::1] output, float[:,:,::1] distances, dict info)\n",
    "    \n",
    "    Runs A1 on a stack of data of a given calendar day; the stack should have one layer (0th dimension) \n",
    "    for each year. The algorithm can be run in a tiled or sliced fashion by passing in a stack \n",
    "    that isn't the full extent. In this case the stack passed should have \"surplus\" data at the edges \n",
    "    so that the neighbourhood search can proceed. The size of these edges should be specified with the \n",
    "    margin parameters. These should be at least as big as the neighbourhood search radius which is set \n",
    "    to 3142 cells area = ~31km radius. \n",
    "    The flags should be the same shape as the data EXCLUDING the margins. \n",
    "    Returns a tuple containing the filled data and distances and fill statistics for logging.\n",
    "    \"\"\"\n",
    "    cdef:\n",
    "        # define all arrays (cython memoryview objects) as being c-contiguous \n",
    "        # so cython doesn't treat as strided (slower access)\n",
    "        # this gives major speedup in access in tight loops as the cpu caching can \n",
    "        # work more effectively\n",
    "        \n",
    "        # intermediate arrays\n",
    "        double [:,::1] nbrTable\n",
    "        int[:,::1] nbrIntCoords\n",
    "        char [:,::1] neverDataLocs\n",
    "        char  noTemplate\n",
    "        # OUTPUTS\n",
    "        float [:,:,::1] outputData\n",
    "        float [:,:,::1] outputDists\n",
    "        unsigned char [:,:,::1] outputFlags\n",
    "        # in-loop working vars\n",
    "        Py_ssize_t z, y \n",
    "        Py_ssize_t zShape, yShape, xShape\n",
    "        Py_ssize_t yShapeTotal, xShapeTotal\n",
    "        int [::1] deltas\n",
    "        double posInf,negInf\n",
    "        \n",
    "        #thread-private ones:\n",
    "        Py_ssize_t x_prv\n",
    "        Py_ssize_t newZ_prv, xi_prv, yi_prv, xNbr_prv, yNbr_prv\n",
    "        int spiralStart_prv\n",
    "        Py_ssize_t max_idx_prv, min_idx_prv\n",
    "        double ws_prv, sw_prv,  pfv_prv, weight_prv, wfv_prv\n",
    "        double altValue_prv, currentValue_prv\n",
    "        int nfound_prv\n",
    "        int delta_prv, deltaidx_prv, nbrIndex_prv\n",
    "        \n",
    "        # intermediate values for ratio-based calculations\n",
    "        # (now just using the same vars whichever approach is used)\n",
    "        #double max_Ratio_prv, maxR_Dist_prv, maxR_wpfv_prv, maxR_weight_prv \n",
    "        #double min_Ratio_prv, minR_Dist_prv, minR_wpfv_prv,  minR_weight_prv\n",
    "        #double ratio_prv\n",
    "        # intermediate values for offset (addition) based calculations\n",
    "        double max_Diff_prv, maxD_Dist_prv, maxD_wpfv_prv, maxD_weight_prv \n",
    "        double min_Diff_prv, minD_Dist_prv, minD_wpfv_prv, minD_weight_prv\n",
    "        double valueDiff_prv\n",
    "        \n",
    "        double sumDist_prv\n",
    "        unsigned char flag_prv\n",
    "        \n",
    "        # First loop stuff\n",
    "        Py_ssize_t xi, yi, x\n",
    "        int nfound\n",
    "        \n",
    "        # Unpack the parameter dictionaries to typed variables \n",
    "        # (the dictionaries were just to clean up the signature a bit in the absence of motivation\n",
    "        # to make some specific type to pass the data more cleanly) \n",
    "        char _OCEAN_FLAG = FlagValues[\"OCEAN\"]\n",
    "        char _FAILURE_FLAG = FlagValues[\"FAILURE\"]\n",
    "        char _SUCCESS_FLAG = FlagValues[\"A1_FILLED\"]\n",
    "        char _SUCCESS_WAS_FULL_FLAG = FlagValues[\"A1_FULL\"]\n",
    "        int marginT = Margins[\"TOP\"]\n",
    "        int marginB = Margins[\"BOTTOM\"]\n",
    "        int marginL = Margins[\"LEFT\"] \n",
    "        int marginR = Margins[\"RIGHT\"]\n",
    "        \n",
    "        float [:,:,::1] dayDataStack = DataStacks[\"Data\"]\n",
    "        unsigned char[:,:,::1] inputFlags = DataStacks[\"Flags\"] # embeds the land-sea mask (sea=1 land =0)\n",
    "        unsigned char[:,:,::1] dataDistTemplate=None\n",
    "        unsigned char[:,::1] knownUnfillableLocs=None\n",
    "        float _AbsZeroPoint = RatioAbsZeroPoint\n",
    "        float _MaxAllowableRatio = RatioLimit\n",
    "        float _MinAllowableRatio = 1.0 / _MaxAllowableRatio\n",
    "        \n",
    "        #  how many locations should be checked in spiral search (gives radius). #3142 \n",
    "        int _MAX_NEIGHBOURS_TO_CHECK = SpiralSearchConfig[\"MAX_NBRS_TO_SEARCH\"] \n",
    "        \n",
    "        # Only use the values gleaned from up to this number of cells (Even if more are avail within radius) #640\n",
    "        int _FILL_THRESHOLD = SpiralSearchConfig[\"MAX_NBRS_REQUIRED\"] \n",
    "        \n",
    "        # 320 min number of values that must be found to have a valid fill\n",
    "        int _FILL_MIN = SpiralSearchConfig[\"MIN_NBRS_REQUIRED\"]\n",
    "        \n",
    "        # calc the distance that is implied by the max spiral search length\n",
    "        int _SEARCH_RADIUS = <int> (sqrt((_MAX_NEIGHBOURS_TO_CHECK*2.0) / 3.14))  + 1 \n",
    "  \n",
    "        # calculation / tracking vars: will be reduction variables - incremented but not read by threads\n",
    "        # for the whole globe there are 933,120,000 pixels per image so this exceeds \n",
    "        # max val of uint after 4 images!\n",
    "        long long totalProcessedGapCells,  totalCells, oceanCells, neverDataCells\n",
    "        long long scannedLevels, scannedNeighbours, usedNeighbours \n",
    "        long long filledBelowThreshold, noPairsFound, insufficientPairsFound, filledToThreshold \n",
    "        long long gapsAtUnfillableLocs, gapsTooBig, dataGood\n",
    "                  \n",
    "    # can precalc dist from gap to nearest data for more  efficient spiral search \n",
    "    # (no need to start spiral search closer than the known nearest data pixel)\n",
    "    if DataStacks[\"DistTemplate\"]:\n",
    "        dataDistTemplate = DataStacks[\"DistTemplate\"]\n",
    "    # can precalc locations where no alternate years exist (so no fill will be possible)\n",
    "    if DataStacks[\"KnownUnfillable\"]:\n",
    "        knownUnfillableLocs = DataStacks[\"KnownUnfillable\"]\n",
    "           \n",
    "    zShape = dayDataStack.shape[0]\n",
    "    # size of the input data including search margins\n",
    "    yShapeTotal = dayDataStack.shape[1]\n",
    "    xShapeTotal = dayDataStack.shape[2]\n",
    "    # size of the data that needs to be filled.\n",
    "    # we will only iterate thru cells that are not in the margins\n",
    "    # (they are equal (zero margin) if filling a global image in one go, and the margin is also zero\n",
    "    # for the slice edges at the edge of the global images)\n",
    "    yShape = dayDataStack.shape[1] - (marginT + marginB)\n",
    "    xShape = dayDataStack.shape[2] - (marginL + marginR)\n",
    "    \n",
    "    # cython doesn't have inf defined \n",
    "    posInf = np.inf\n",
    "    negInf = -np.inf\n",
    "      \n",
    "    # generate nbr distance table (offset coordinates for the spiral search steps) using numpy\n",
    "    diam = _SEARCH_RADIUS * 2 + 1\n",
    "    inds = np.indices([diam,diam]) - _SEARCH_RADIUS\n",
    "    distTmp = np.sqrt((inds ** 2).sum(0))\n",
    "    npTmpTable = ((inds.T).reshape(diam**2, 2))\n",
    "    npTmpTable = np.append(npTmpTable, distTmp.ravel()[:,None],axis=1)\n",
    "    # sort the table by distance then x then y (the arguments are last-sort-first)\n",
    "    order = np.lexsort((npTmpTable[:,1], npTmpTable[:,0], npTmpTable[:,2]))\n",
    "    npTmpTable = np.take(npTmpTable, order, axis=0)\n",
    "    \n",
    "    # the C-side result of the distance table calculations\n",
    "    # transpose it to have three rows and many columns and take a C contiguous copy of this\n",
    "    # so that cython can access individual nbr coord sets more quickly\n",
    "    nbrTable = np.copy((npTmpTable[npTmpTable[:,2] <= _SEARCH_RADIUS]).T,order='c')\n",
    "    \n",
    "    # the distance table is stored with a float type (for the distances) but we need ints \n",
    "    # for indexing based on coords. we can cast at the time we get them out, but as this happens \n",
    "    # in the innermost loop it is done trillions of times and so the time penalty of that \n",
    "    # becomes signficant. So, store an int version of the coords array\n",
    "    nbrIntCoords = np.asarray(nbrTable[0:2,:]).astype(np.int32)\n",
    "    \n",
    "    # initialise C arrays each from an appropriate empty numpy object\n",
    "    # these will have the same size as the area we're actually filling i.e. not the (buffered)\n",
    "    # input section\n",
    "    outputData = np.empty(shape=(zShape, yShape, xShape), dtype='float32', order='c')\n",
    "    outputDists = np.empty(shape=(zShape, yShape, xShape), dtype='float32', order='c')\n",
    "    outputFlags = np.zeros(shape=(zShape, yShape, xShape), dtype='uint8', order='c')\n",
    "    outputData[:] = _NDV\n",
    "    outputDists[:] = _NDV\n",
    "    \n",
    "    # handle optional parameters\n",
    "    if dataDistTemplate is None:\n",
    "        noTemplate = 1 \n",
    "    else:\n",
    "        noTemplate = 0\n",
    "        assert (dataDistTemplate.shape[0] == zShape and\n",
    "                dataDistTemplate.shape[1] == yShape and \n",
    "                dataDistTemplate.shape[2] == xShape\n",
    "                )\n",
    "    \n",
    "    # diagnostics; TODO use logging\n",
    "    print (\"Running A1 (Full Spiral Search).\")\n",
    "    print (\"No data template: {0!s}. Using ratio method: {1!s}. Searching for {2!s} - {3!s} nbrs within {4!s} spiral steps\".\n",
    "           format(noTemplate, FillByRatios, _FILL_MIN, _FILL_THRESHOLD, _MAX_NEIGHBOURS_TO_CHECK))\n",
    "    print (\"Calculating nbr table out to radius of {0!s}.\".format(_SEARCH_RADIUS))\n",
    "    print (\"Filling from stack position {0!s}.\".format(RunFillFromPos))\n",
    "                                                                      \n",
    "    assert inputFlags.shape[1] == yShapeTotal\n",
    "    assert inputFlags.shape[2] == xShapeTotal\n",
    "    \n",
    "    if knownUnfillableLocs is None: # i.e. has not been precalced with np.all\n",
    "        # knownUnfillableLocs will a 2D map of cells where there is no data in any year (z axis)\n",
    "        knownUnfillableLocs = np.ones(shape=(yShape, xShape), dtype=np.uint8, order='c')\n",
    "        # locate cells that CAN be filled (negating it like this means we can keep \n",
    "        # the loop order cache-friendly - as opposed to iterating through Z at each \n",
    "        # y,x location until/unless we find a value > 0. \n",
    "        # This process is still slower on 1 thread than numpy.all, but not significant overall,\n",
    "        # and multithreading makes it way faster to do here\n",
    "        with nogil:\n",
    "            for z in range(zShape):\n",
    "                # work is trivial and fairly well balanced so use static schedule\n",
    "                for y in prange(yShape, schedule='static', num_threads=20): \n",
    "                    x_prv = -1\n",
    "                    yi_prv = y + marginT\n",
    "                    for x_prv in range(xShape):\n",
    "                        xi_prv = x_prv + marginL\n",
    "                        if dayDataStack[z, yi_prv, xi_prv] != _NDV:\n",
    "                            knownUnfillableLocs[y, x_prv] = 0\n",
    "    else:\n",
    "        assert (knownUnfillableLocs.shape[0] == yShape and\n",
    "                knownUnfillableLocs.shape[1] == xShape\n",
    "                )\n",
    "    \n",
    "    if FillByRatios:\n",
    "        # we need to work with ratios between cells but will be running this on non-ratio scale variables\n",
    "        # (i.e. where 0 is an arbitrary point) such as temp in celsius. This could lead to div/0 giving infinite\n",
    "        # ratios, and it's not valid anyway (e.g. 20/10 != 10/-10 even though the interval is the same).\n",
    "        # The latter point doesn't matter _much_ because we only use the ratio to multiply back against an alternate\n",
    "        # value which will be similar. Use a large \"absolute zero\" relative to the values and it matters even less\n",
    "        # (but not too large, to avoid FP errors)\n",
    "        with nogil:\n",
    "            for z in range(zShape):\n",
    "                for y in prange(yShapeTotal, schedule='static', num_threads=20):\n",
    "                    x_prv = -1\n",
    "                    for x_prv in range(xShapeTotal):\n",
    "                        if dayDataStack[z, y, x_prv] != _NDV:\n",
    "                            dayDataStack[z, y, x_prv] = dayDataStack[z, y, x_prv] -  _AbsZeroPoint\n",
    "    else:\n",
    "        _AbsZeroPoint = 0 # to avoid need for further check in loop below\n",
    "        \n",
    "    # initialise metrics\n",
    "    totalProcessedGapCells = 0 # for testing\n",
    "    totalCells = 0 # for testing\n",
    "    scannedLevels = 0 # for testing\n",
    "    scannedNeighbours = 0 # for testing\n",
    "    filledToThreshold = 0 \n",
    "    filledBelowThreshold=0\n",
    "    noPairsFound = 0\n",
    "    insufficientPairsFound = 0\n",
    "    \n",
    "    gapsAtUnfillableLocs = 0\n",
    "    gapsTooBig=0\n",
    "    usedNeighbours = 0\n",
    "    gapsInKnownUnfillable = 0\n",
    "    dataGood = 0\n",
    "    oceanCells = 0\n",
    "    neverDataCells = 0\n",
    "    \n",
    "    # a single call to produce the alternating forward/back year offsets\n",
    "    deltas = alternates_cy(zShape)\n",
    "    \n",
    "    # now do A1 gap fill! Iterate through the entire data stack \n",
    "    # The arrays are C-ordered so it's fastest in terms of CPU cache to have \n",
    "    # X on the innermost loop. Parallelise over the y axis.\n",
    "    \n",
    "    # Telling cython how to parallelise the variables is done implicitly by how you access them.\n",
    "    # - assign to a variable if you want it to be thread-private i.e. x = x + 1\n",
    "    # - increase it in-place if you want it to be shared i.e. x += 1\n",
    "    # The latter case turns it into a \"reduction\" variable which cannot be read in the parallel \n",
    "    # loop. \n",
    "    # Therefore all variables that need to be thread private must be made so by artifially assigning \n",
    "    # (something) to them within the parallel block. \n",
    "    # Check the generated C code to be sure it's worked as intended!\n",
    "    with nogil, parallel(num_threads=10): # change num cores here\n",
    "        for z in range(zShape):\n",
    "            if z >= RunFillFromPos:\n",
    "                # experiments with chunksizes 500, 50, 20, 2, and parallelising on z axis instead,\n",
    "                # showed 20 to be the quickest (tradeoff between allocation overhead vs one thread doing \n",
    "                # a \"gappy\" area all by itself). It's likely to vary by machine and num threads due to \n",
    "                # CPU cache differences i.e. the spiral search needs a lot of cache to be efficient\n",
    "                # as it crosses rows so the access cannot be completely cache-economical. \n",
    "                # However 6 threads was still slower than 12 on a 6-physical 12-virtual machine\n",
    "                for y in prange(yShape, schedule='dynamic', chunksize=20):\n",
    "                    # assign something to all variables that are used in filling one cell and need \n",
    "                    # to be private (i.e. they get modified but we don't want the iteration that fills \n",
    "                    # another cell to know about that).\n",
    "                    # It's verbose but this tricks cython into making them thread-private\n",
    "                    nfound_prv = 0\n",
    "                    ws_prv = 0\n",
    "                    sw_prv = 0\n",
    "                    weight_prv=0\n",
    "                    pfv_prv=0\n",
    "                    #ratio_prv=0\n",
    "                    valueDiff_prv = 0\n",
    "                    delta_prv=0\n",
    "                    deltaidx_prv = -1\n",
    "                    currentValue_prv=0\n",
    "                    altValue_prv=0\n",
    "                    xNbr_prv=-1\n",
    "                    yNbr_prv=-1\n",
    "                    xi_prv=-1\n",
    "                    yi_prv=-1\n",
    "                    x_prv=-1\n",
    "                    newZ_prv=-1\n",
    "                    nbrIndex_prv=-1\n",
    "                    sumDist_prv=0\n",
    "\n",
    "                    max_Diff_prv = negInf\n",
    "                    maxD_Dist_prv = 0\n",
    "                    maxD_weight_prv=0\n",
    "                    maxD_wpfv_prv=0\n",
    "                    \n",
    "                    min_Diff_prv = posInf\n",
    "                    minD_Dist_prv = 0\n",
    "                    minD_wpfv_prv = 0\n",
    "                    minD_weight_prv = 0\n",
    "                    \n",
    "                    max_idx_prv = 0\n",
    "                    min_idx_prv = 0\n",
    "                    flag_prv = -1\n",
    "                    \n",
    "                    yi_prv = y + marginT\n",
    "                    \n",
    "                    for x_prv in range(xShape):\n",
    "                        xi_prv = x_prv + marginL\n",
    "                        #re initialise variables for this cell\n",
    "                        nfound_prv= 0\n",
    "                        ws_prv = 0.0\n",
    "                        sw_prv = 0\n",
    "                        weight_prv = 0\n",
    "                        pfv_prv = 0\n",
    "                        #ratio_prv = 0\n",
    "                        valueDiff_prv = 0\n",
    "                        delta_prv = 0\n",
    "                        deltaidx_prv = -1\n",
    "                        currentValue_prv = 0\n",
    "                        altValue_prv = 0\n",
    "                        xNbr_prv = -1\n",
    "                        yNbr_prv = -1\n",
    "                        newZ_prv = -1\n",
    "                        nbrIndex_prv = -1\n",
    "                        sumDist_prv = 0\n",
    "\n",
    "                        max_Diff_prv = negInf\n",
    "                        maxD_Dist_prv = 0\n",
    "                        maxD_weight_prv = 0\n",
    "                        maxD_wpfv_prv = 0\n",
    "\n",
    "                        min_Diff_prv = posInf\n",
    "                        minD_Dist_prv = 0\n",
    "                        minD_wpfv_prv = 0\n",
    "                        minD_weight_prv = 0\n",
    "\n",
    "                        max_idx_prv = 0\n",
    "                        min_idx_prv = 0\n",
    "                        flag_prv = -1\n",
    "                    \n",
    "                        totalCells += 1 # testing log\n",
    "                        \n",
    "                        if ((inputFlags[z, yi_prv, xi_prv] & _OCEAN_FLAG) == _OCEAN_FLAG):\n",
    "                            oceanCells += 1\n",
    "                            outputFlags[z, y, x_prv] = inputFlags[z, yi_prv, xi_prv]\n",
    "                            #in the ocean do not copy across (even if there is data; MODIS may not match shore cleanly)\n",
    "                            #output is already nodata and flag is alraady set so just \n",
    "                            continue\n",
    "                        \n",
    "                        if ((inputFlags[z, yi_prv, xi_prv] & _FAILURE_FLAG) == _FAILURE_FLAG):\n",
    "                            # also do this if failure flag (2) is set (by despeckle algorithm indicating that mean \n",
    "                            # was ND thus never any data on any calendar day)\n",
    "                            outputFlags[z, y, x_prv] = inputFlags[z, yi_prv, xi_prv]\n",
    "                            neverDataCells +=1\n",
    "                            continue\n",
    "\n",
    "                        currentValue_prv = dayDataStack[z, yi_prv, xi_prv]\n",
    "\n",
    "                        #if value is valid just copy it across\n",
    "                        if not currentValue_prv == _NDV: # 0.0 is valid!!\n",
    "                            # we have data, copy it across - output does not have margins\n",
    "                            outputData[z, y, x_prv] = currentValue_prv + _AbsZeroPoint\n",
    "                            outputFlags[z, y, x_prv] = inputFlags[z, yi_prv, xi_prv]\n",
    "                            dataGood += 1\n",
    "                            # flag is already non-ocean so just\n",
    "                            continue\n",
    "\n",
    "                        # if it's a location that has no data in any alternate year of this calendar day\n",
    "                        # then we also cannot fill\n",
    "                        if knownUnfillableLocs[y, x_prv] == 1:\n",
    "                            totalProcessedGapCells += 1\n",
    "                            gapsAtUnfillableLocs += 1\n",
    "                            # for testing to check failure reason set this to 3?\n",
    "                            outputFlags[z, y, x_prv] = outputFlags[z, y, x_prv] | _FAILURE_FLAG\n",
    "                            # leave output at _NDV and just \n",
    "                            continue\n",
    "\n",
    "                        # if we have a precalculated \"large gaps\" template does this show that \n",
    "                        # the cell will be unfillable due to lack of neighbours?\n",
    "                        # The template gives locations that are more than _SEARCH_RADIUS\n",
    "                        # away from any data point in the same year, i.e. where the spiral search will not\n",
    "                        # be able to find anything. Precalculating this in scipy is generally somewhat faster \n",
    "                        # than leaving the code to iterate through everything here IF we only use 1 core\n",
    "                        # NB sc evaluation makes this safe whether or not dataDistTemplate exists\n",
    "                        if (noTemplate == 0 and dataDistTemplate[z, yi_prv, xi_prv] > _SEARCH_RADIUS + 1):\n",
    "                            # TODO set a flag for this\n",
    "                            outputFlags[z, y, x_prv] = outputFlags[z, y, x_prv] | _FAILURE_FLAG\n",
    "                            totalProcessedGapCells += 1\n",
    "                            gapsTooBig += 1\n",
    "                            continue\n",
    "\n",
    "                        # else attempt to fill the gap, using A1\n",
    "                        if 1: # can't be bothered to rejig indents\n",
    "                            # METRICS\n",
    "                            totalProcessedGapCells += 1\n",
    "                            flag_prv = inputFlags[z, yi_prv, xi_prv]\n",
    "                            # deltas will be (1, -1, 2, -2, ...)\n",
    "                            for deltaidx_prv in range(0, deltas.shape[0]):\n",
    "                                delta_prv = deltas[deltaidx_prv]\n",
    "                                newZ_prv = z + delta_prv\n",
    "                                # does this zDelta fall off the start or end of the stack?\n",
    "                                # or is the alternate year also blank here?\n",
    "                                # (or, obviously, are we checking the current year!)\n",
    "                                if (newZ_prv >= zShape or newZ_prv < 0 \n",
    "                                    or newZ_prv == z\n",
    "                                    or dayDataStack[newZ_prv, yi_prv, xi_prv] == _NDV):\n",
    "                                    continue\n",
    "                                # otherwise...\n",
    "                                scannedLevels += 1 # tracking year-switches\n",
    "                                \n",
    "                                # note that the alternate value can be zero. on a ratio based fill \n",
    "                                # this would result in a fill value of zero, too, no matter what the nbr values are\n",
    "                                altValue_prv = dayDataStack[newZ_prv, yi_prv, xi_prv]\n",
    "                                \n",
    "                                #step thru spiral-outward coords table\n",
    "                                spiralStart_prv = 1 # zero is this cell\n",
    "                                # if possible, check where to start the search based on known closest data\n",
    "                                if noTemplate == 0:\n",
    "                                    spiralStart_prv = <int>(((dataDistTemplate[z, yi_prv, xi_prv]-1)**2) * 3.141)\n",
    "                                    if spiralStart_prv < 1:\n",
    "                                        spiralStart_prv = 1\n",
    "                                    elif spiralStart_prv > _MAX_NEIGHBOURS_TO_CHECK:\n",
    "                                        spiralStart_prv = _MAX_NEIGHBOURS_TO_CHECK\n",
    "                                \n",
    "                                # spiral search for this alternate year\n",
    "                                for nbrIndex_prv in range(spiralStart_prv, _MAX_NEIGHBOURS_TO_CHECK + 1):\n",
    "                                    #scannedNeighbours += 1 # don't track as it's unnecessary op in inner loop\n",
    "\n",
    "                                    # This is the inner loop that runs once for every neighbour of \n",
    "                                    # every gap cell, i.e. potentially many trillions of times.\n",
    "                                    # So it's really important to get this loop fast.\n",
    "\n",
    "                                    # Total time of this loop where the if below fails is now ~4.7nS\n",
    "                                    # or around 12 CPU cycles. Considering there are potentially 4 array\n",
    "                                    # accesses and several comparisons this is probably as good as we can \n",
    "                                    # get and it's clear that the L1D CPU cache is mostly being hit (as a \n",
    "                                    # single access to L2 cache is ~14 cycles)\n",
    "\n",
    "                                    # moving int cast outside the loop saves \n",
    "                                    # ~ 2.7nS per loop i.e. >50% on an invalid iteration\n",
    "                                    xNbr_prv = xi_prv + nbrIntCoords[0, nbrIndex_prv]\n",
    "                                    yNbr_prv = yi_prv + nbrIntCoords[1, nbrIndex_prv]\n",
    "\n",
    "                                    # is the requested neighbour cell within data bounds?\n",
    "                                    # (allowing for the fact that the input data can have greater \n",
    "                                    # extent than the cells we are searching to allow tiled processing)\n",
    "                                    # and does it have data in both relevant years?\n",
    "                                    # Put current year check before alternate year check as current year is more\n",
    "                                    # likely to not have data (given we are already at a gap) so it's more \n",
    "                                    # efficient to bail out there first\n",
    "                                    if (xNbr_prv >= 0 and xNbr_prv < xShapeTotal and \n",
    "                                            yNbr_prv >= 0 and yNbr_prv < yShapeTotal and\n",
    "                                            dayDataStack[z, yNbr_prv, xNbr_prv] != _NDV and \n",
    "                                            dayDataStack[newZ_prv, yNbr_prv, xNbr_prv] != _NDV):\n",
    "                                        # we're good to go! do the maths for this contributing cell\n",
    "                                        usedNeighbours +=  1 # tracking\n",
    "                                        \n",
    "                                        if FillByRatios == 0:\n",
    "                                        #calculate difference not ratio\n",
    "                                            valueDiff_prv = (dayDataStack[z, yNbr_prv, xNbr_prv] \n",
    "                                                             - dayDataStack[newZ_prv, yNbr_prv, xNbr_prv])\n",
    "                                            pfv_prv = altValue_prv + valueDiff_prv\n",
    "                                        else:\n",
    "                                            # calculate ratio, taking some precautions to avoid stupid values \n",
    "                                            # when one is close to zero\n",
    "                                            valueDiff_prv = (dayDataStack[z, yNbr_prv, xNbr_prv] \n",
    "                                                             / dayDataStack[newZ_prv, yNbr_prv, xNbr_prv])\n",
    "                                            if (dayDataStack[newZ_prv, yNbr_prv, xNbr_prv] == 0 \n",
    "                                                or valueDiff_prv > _MaxAllowableRatio):\n",
    "                                                valueDiff_prv = _MaxAllowableRatio\n",
    "                                            elif valueDiff_prv < _MinAllowableRatio:\n",
    "                                                valueDiff_prv = _MinAllowableRatio\n",
    "                                            pfv_prv = altValue_prv * valueDiff_prv + _AbsZeroPoint\n",
    "                                        # implicit assumption that one year and one cell distance have the same weighting\n",
    "                                        weight_prv = (1.0 / abs(delta_prv)) * (1.0 / nbrTable[2,nbrIndex_prv])\n",
    "                                        ws_prv += pfv_prv * weight_prv\n",
    "                                        sw_prv += weight_prv\n",
    "                                        # the only thing the replacement values array got used for in original IDL was \n",
    "                                        # calculating the mean contibuting distance. we don't need to \n",
    "                                        # do all those assignments for that\n",
    "                                        sumDist_prv = sumDist_prv + nbrTable[2,nbrIndex_prv]\n",
    "                                        # track the things we need to trim min/max\n",
    "                                        if valueDiff_prv < min_Diff_prv:\n",
    "                                            min_Diff_prv = valueDiff_prv\n",
    "                                            minD_Dist_prv = nbrTable[2,nbrIndex_prv]\n",
    "                                            minD_wpfv_prv = pfv_prv*weight_prv\n",
    "                                            minD_weight_prv = weight_prv\n",
    "                                        if valueDiff_prv > max_Diff_prv:\n",
    "                                            max_Diff_prv = valueDiff_prv\n",
    "                                            maxD_Dist_prv = nbrTable[2,nbrIndex_prv]\n",
    "                                            maxD_wpfv_prv = pfv_prv*weight_prv\n",
    "                                            maxD_weight_prv = weight_prv\n",
    "                                        nfound_prv = nfound_prv + 1\n",
    "                                        #nfound = nfound + 1\n",
    "                                        if nfound_prv == _FILL_THRESHOLD:\n",
    "                                            break\n",
    "                                if nfound_prv == _FILL_THRESHOLD:\n",
    "                                    break\n",
    "                            \n",
    "                            if nfound_prv >= _FILL_MIN:\n",
    "                                # we have found between min and threshold pairs\n",
    "                                flag_prv = flag_prv | _SUCCESS_FLAG\n",
    "                                if nfound_prv == _FILL_THRESHOLD:\n",
    "                                    # we have found the upper limit of pairs (full fill)\n",
    "                                    # (set both success flags)\n",
    "                                    filledToThreshold += 1\n",
    "                                    flag_prv = flag_prv | _SUCCESS_WAS_FULL_FLAG\n",
    "                                else:\n",
    "                                    filledBelowThreshold += 1\n",
    "                                if _TRIM_MIN_MAX:\n",
    "                                    ws_prv = ws_prv - (minD_wpfv_prv + maxD_wpfv_prv)\n",
    "                                    sw_prv = sw_prv - (minD_weight_prv + maxD_weight_prv)\n",
    "                                    sumDist_prv = sumDist_prv - (minD_Dist_prv + maxD_Dist_prv)\n",
    "                                    nfound_prv = nfound_prv - 2\n",
    "                                    #flag = 12 (old code recorded this per-pixel, don't see why)\n",
    "                                wfv_prv = ws_prv / sw_prv\n",
    "                                # record the result!!\n",
    "                                outputData[z, y, x_prv] = wfv_prv\n",
    "                                outputDists[z, y, x_prv] = sumDist_prv / nfound_prv\n",
    "                            else:\n",
    "                                flag_prv = flag_prv | _FAILURE_FLAG\n",
    "                                # do not do anything to output: leave it at no-data\n",
    "                                if nfound_prv > 0:\n",
    "                                    insufficientPairsFound += 1\n",
    "                                else:\n",
    "                                    noPairsFound += 1\n",
    "                            outputFlags [z, y, x_prv] = flag_prv\n",
    "    print (\"Total cells scanned:     \"+str(totalCells))\n",
    "    print (\"Total cells with good data: \"+str(dataGood))\n",
    "    print (\"Total cells ocean: \"+str(oceanCells))\n",
    "    print (\"Total cells never-data: \"+str(neverDataCells))\n",
    "    print (\"Total processed gaps:    \"+str(totalProcessedGapCells))\n",
    "    print (\"Total gaps too large to fill: \"+str(gapsTooBig))\n",
    "    print (\"Total perma-gaps:        \"+str(gapsAtUnfillableLocs))\n",
    "    print (\"Total gaps filled fully: \"+str(filledToThreshold))\n",
    "    print (\"Total filled partially:  \"+str(filledBelowThreshold))\n",
    "    print (\"Total w/insufficient prs:\"+str(insufficientPairsFound))\n",
    "    print (\"Total with no nbr pairs: \"+str(noPairsFound))\n",
    "    print (\"Total yrs scanned f/b:   \"+str(scannedLevels))\n",
    "    print (\"Total nbr cells scanned: \"+str(scannedNeighbours))\n",
    "    print (\"Total used nbr pairs:    \"+str(usedNeighbours))\n",
    "    \n",
    "    objRes = {\n",
    "        \"TotalCells\":totalCells,\n",
    "        \"Ocean\":oceanCells,\n",
    "        \"NeverData\":neverDataCells,\n",
    "        \"GoodCells\":dataGood,\n",
    "        \"TotalGaps\":totalProcessedGapCells,\n",
    "        \"PermaGaps\":gapsAtUnfillableLocs,\n",
    "        \"FilledFull\":filledToThreshold,\n",
    "        \"FilledPartial\":filledBelowThreshold,\n",
    "        \"FailInsufficientPairs\":insufficientPairsFound,\n",
    "        \"FailNoPairs\":noPairsFound,\n",
    "        \"TotalAlternateYrs\":scannedLevels,\n",
    "        \"TotalNbrsChecked\":scannedNeighbours,\n",
    "        \"TotalNbrsUsed\":usedNeighbours\n",
    "    }\n",
    "    #return (output,flags)# and dists!\n",
    "    return (outputData,outputDists,outputFlags,objRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 core algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=/openmp --link-args=/openmp --force\n",
    "#%%cython --compile-args=-fopenmp --link-args=-fopenmp --force\n",
    "import numpy as np\n",
    "cimport cython \n",
    "from libc.math cimport sqrt\n",
    "#cdef int _SEARCH_RADIUS = 10\n",
    "from cython.parallel import prange, parallel\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "cpdef fillGapsA2_Cy (\n",
    "            dict DataImages,\n",
    "            dict FlagValues,          \n",
    "            float _NDV,\n",
    "            Py_ssize_t _A2_MAX_NBRS,\n",
    "            char FillByRatios = 0,\n",
    "            float RatioAbsZeroPoint = 0,\n",
    "            float RatioLimit = 1\n",
    "            ):\n",
    "    '''\n",
    "    Cython (C) implementation of the A2 gapfilling algorithm main \"pass\" code. This function should be called 8 \n",
    "    times with the data structured in such a way that the 8 different directional pass fills are generated, in \n",
    "    order to generate overall A2 fill results. A separate (python) function is provided for this.\n",
    "    \n",
    "    The nature of the A2 algorithm is such that it cannot be easily parallelised - it modifies the input based \n",
    "    on results from neighbouring cells, so the cells must be run in a deterministic order. \n",
    "    \n",
    "    Likewise A2 \"drags\" fill value ratios / calculations for an unlimited distance from nearest data pixels. This \n",
    "    means that it cannot be run on separate tiles like A1 and must be run on global images.\n",
    "    \n",
    "    These two factors mean that unlike in the published paper, A2 is both slower and more demanding of memory than A1,\n",
    "    whilst producing less good results. \n",
    "    \n",
    "    However it is still required if we need to be assured that _all_ gaps will be filled because A1 only works out to \n",
    "    a specified radius (gap size). (In future we may try iterative / repeated running of A1 until all gaps are filled)\n",
    "    \n",
    "    This implementation is optimised as far as it can be in terms of Cython optimisations, except for the approach taken \n",
    "    to the 8 different directional passes. To reduce memory use, the caller function (elsewhere) does not not make a \n",
    "    C-ordered copy of the data but re-strides it. This greatly slows the A2 function (by a factor of around 6) and so \n",
    "    on a machine with sufficent memory the data should be copied into the right order before passing to this function.\n",
    "    '''\n",
    "    # We assume that the arrays passed in are strided / transposed \n",
    "    # such that iterating through in the standard c-normal order \n",
    "    # will go through the source data in the correct order for this \n",
    "    # directional pass. This means that some passes (the ones where the data \n",
    "    # are in the native order) are several times quicker than the others \n",
    "    cdef:\n",
    "        # intermediate arrays\n",
    "        double [:,::1] nbrTable\n",
    "        int[:,::1] nbrIntCoords\n",
    "        \n",
    "        float[:,::1] diffImage_Local\n",
    "        float [:, :] origDistImage_LocalCopy\n",
    "        # in loop working vars\n",
    "        Py_ssize_t y, x, yShape, xShape, nbrIndex, xNbr, yNbr\n",
    "        double nbrDiffSum, nbrDiffCount, nbrDistSum, diffValThisPass\n",
    "        double local_distance, nbrDist\n",
    "        \n",
    "        # metrics\n",
    "        long long gotPixelVals = 0\n",
    "        \n",
    "        # unpack inputs to typed variables \n",
    "        float[:,:] dataImage_Global_R = DataImages[\"Data\"] # global in both senses\n",
    "        unsigned char[:,:] flagsImage_Global_R = DataImages[\"Flags\"]\n",
    "        float[:,:] origDistImage_Global_R = DataImages[\"Distances\"]\n",
    "        float[:,:] meanImage_Global_R = DataImages[\"Means\"]\n",
    "        # these inputs get modified, i.e. they are \"out\" parameters in a proper language\n",
    "        # It is done like this rather than having return values because they are actually going to be\n",
    "        # strided views on arrays (to change iteration order)\n",
    "        float[:,:] sumDistImage_Global_W = DataImages[\"SumDist\"] \n",
    "        float[:,:] outputImage_ThisPass_W = DataImages[\"Output\"]\n",
    "        char _FILL_FAILED_FLAG = FlagValues[\"FAILURE\"]\n",
    "        char _OCEAN_FLAG = FlagValues[\"OCEAN\"]\n",
    "        \n",
    "        float _AbsZeroPoint = RatioAbsZeroPoint\n",
    "        float _MaxAllowableRatio = RatioLimit\n",
    "        float _MinAllowableRatio = 1.0 / _MaxAllowableRatio\n",
    "        \n",
    "    yShape = dataImage_Global_R.shape[0]\n",
    "    xShape = dataImage_Global_R.shape[1]\n",
    "            \n",
    "    # it's actually the max neigbours value that defines how far out the search runs. \n",
    "    # this just makes sure that the nbr table is generated far enough out.\n",
    "    _SEARCH_RADIUS = <int>sqrt(_A2_MAX_NBRS / 3.14) + 10\n",
    "    diam = _SEARCH_RADIUS * 2 + 1\n",
    "    inds = np.indices([diam,diam]) - _SEARCH_RADIUS\n",
    "    distTmp = np.sqrt((inds ** 2).sum(0))\n",
    "    npTmpTable = ((inds.T).reshape(diam**2, 2))\n",
    "    npTmpTable = np.append(npTmpTable, distTmp.ravel()[:,None],axis=1)\n",
    "\n",
    "    # sort the table by distance then x then y (the arguments are last-sort-first)\n",
    "    order = np.lexsort((npTmpTable[:,1],npTmpTable[:,0],npTmpTable[:,2]))\n",
    "    npTmpTable = np.take(npTmpTable,order,axis=0)\n",
    "\n",
    "    # the C-side result of the distance table calculations\n",
    "    # transpose it to have three rows and many columns and take a C contiguous copy of this\n",
    "    # so that access to individual nbr coord sets is optimised\n",
    "    nbrTable = np.copy((npTmpTable[npTmpTable[:,2] <= _SEARCH_RADIUS]).T,order='c')\n",
    "    # the distance table is stored with a float type but we need ints for indexing\n",
    "    # based on its coords. we can cast at the time we get them out, but as this happens \n",
    "    # in the innermost loop it is done trillions of times and so the time penalty of that \n",
    "    # becomes signficant. So, store an int version of the coords array\n",
    "    nbrIntCoords = np.asarray(nbrTable[0:2,:]).astype(np.int32)\n",
    "    \n",
    "    print (\"Beginning pass of A2...\")\n",
    "    # populate the ratio or difference image, \n",
    "    # this is just (data / mean) or (data - mean) (whether the data are original or from A1)\n",
    "    diffImage_Local = np.empty_like(dataImage_Global_R)\n",
    "    diffImage_Local[:] = _NDV\n",
    "    with nogil, parallel(num_threads=20):\n",
    "        #outerIdx = -1\n",
    "        for y in prange(0, yShape):\n",
    "            x = -1\n",
    "            for x in range(0, xShape):\n",
    "                if ((flagsImage_Global_R[y, x] & _OCEAN_FLAG) == _OCEAN_FLAG\n",
    "                    # or meanImage_Global_R[y, x] == 0 # we need to be able to cope with mean = 0\n",
    "                    or meanImage_Global_R[y, x] == _NDV\n",
    "                    or dataImage_Global_R[y, x] == _NDV\n",
    "                    ):\n",
    "                    continue\n",
    "                if FillByRatios == 0:    \n",
    "                    diffImage_Local[y, x] = (dataImage_Global_R[y, x] - meanImage_Global_R[y, x])\n",
    "                else:\n",
    "                    diffImage_Local[y, x] =((dataImage_Global_R[y, x] - _AbsZeroPoint)\n",
    "                        / (meanImage_Global_R[y, x] - _AbsZeroPoint))\n",
    "                    if diffImage_Local[y, x] > _MaxAllowableRatio:\n",
    "                        diffImage_Local[y, x] = _MaxAllowableRatio\n",
    "                    elif diffImage_Local[y, x] < _MinAllowableRatio:\n",
    "                        diffImage_Local[y, x] = _MinAllowableRatio\n",
    "    \n",
    "                \n",
    "    # the distances image gets modified within a pass but we don't want / need\n",
    "    # to store it outside of the pass\n",
    "    # However we are now creating the fresh copy in the caller function for clarity\n",
    "    #origDistImage_LocalCopy = np.copy(origDistImage_Global_R)\n",
    "    origDistImage_LocalCopy = origDistImage_Global_R\n",
    "    \n",
    "    with nogil:\n",
    "        for y in range (0, yShape): # can't do parallel here,  boooo\n",
    "            for x in range (0, xShape):\n",
    "                #flag = flagsImage[y,x]\n",
    "                if (flagsImage_Global_R[y, x] & _FILL_FAILED_FLAG) != _FILL_FAILED_FLAG:\n",
    "                    # it's already good data, or filled with A1\n",
    "                    continue\n",
    "                if meanImage_Global_R[y,x] == _NDV:\n",
    "                    #we can't fill, but, if we are here then the flag is already set\n",
    "                    #to failure (from A1)... could optionally set a separate A2 failure flag (128)\n",
    "                    continue\n",
    "                # else...\n",
    "                nbrDiffSum = 0\n",
    "                nbrDiffCount = 0\n",
    "                nbrDistSum = 0\n",
    "                # summarise the values in the surrounding 8 pixels \n",
    "                # we cannot precalculate this elementwise because the \n",
    "                # offsets / ratios in diffImageThisPass are updated in the loop,\n",
    "                # affecting later iterations, hence why we can't parallelise simply\n",
    "                for nbrIndex in range(1, _A2_MAX_NBRS+1):\n",
    "                    # +1 because the first row of nbr table is the current cell\n",
    "                    # this was an attempt at loop reversal so we could always keep it c-contiguous - \n",
    "                    # specify the order of the inner / outer loop as a parameter. Never got it working yet...\n",
    "                    #if outerLoopShouldBe == 0: \n",
    "                    xNbr = x + nbrIntCoords[0, nbrIndex]\n",
    "                    yNbr = y + nbrIntCoords[1, nbrIndex]\n",
    "                    if (xNbr >= 0 and xNbr < xShape and\n",
    "                            yNbr >= 0 and yNbr < yShape and\n",
    "                            diffImage_Local[yNbr, xNbr] != _NDV):\n",
    "                        nbrDiffSum += diffImage_Local[yNbr, xNbr]\n",
    "                        nbrDiffCount += 1\n",
    "                        if origDistImage_LocalCopy[yNbr, xNbr] != _NDV:\n",
    "                            nbrDist = origDistImage_LocalCopy[yNbr, xNbr] \n",
    "                            # the distance of the filled cell will be the average of the distances\n",
    "                            # of the neighbour cells used. Where \"distance\" of a neighbour cell \n",
    "                            # will be the physical distance from the working cell, plus the \n",
    "                            # distance already associated with the filled value in the nbr cell\n",
    "                            # from the A1 algorithm, if applicable.\n",
    "                            nbrDistSum += nbrDist\n",
    "                        nbrDistSum += nbrTable[2, nbrIndex]\n",
    "                        \n",
    "                # if any of the surrounding 8 pixels had a value then derive the cell \n",
    "                # value from it\n",
    "                if nbrDiffCount > 0 and (FillByRatios == 0 or nbrDiffSum > 0):\n",
    "                    gotPixelVals += 1\n",
    "                    # ratio / diff to use is the mean of the (up to) 8 values surrounding the cell\n",
    "                    diffValThisPass = nbrDiffSum / nbrDiffCount\n",
    "                    # fill in the same ratio image that we are checking in the neigbour search\n",
    "                    # step such that the next cell along, in the current pass direction and if \n",
    "                    # it's a gap, will have this calculated value as an available input.\n",
    "                    # Hence values are \"smeared\" in direction of the scan.\n",
    "                    # This means loop iterations are not independent (not embarrasingly \n",
    "                    # parallel!) and thus this algorithm needs to be run on an entire \n",
    "                    # global image. \n",
    "                    diffImage_Local[y, x] = diffValThisPass\n",
    "                    if FillByRatios == 0:\n",
    "                        outputImage_ThisPass_W[y, x] = (diffValThisPass + meanImage_Global_R[y, x])\n",
    "                    else:\n",
    "                        outputImage_ThisPass_W[y, x] = (diffValThisPass * \n",
    "                                                        (meanImage_Global_R[y, x] - _AbsZeroPoint) \n",
    "                                                    + _AbsZeroPoint)\n",
    "                else:\n",
    "                    outputImage_ThisPass_W[y, x] = _NDV\n",
    "                    # outside the function we can fill in flags image with _UNFILLABLE_AT_A2\n",
    "                    # if all passes are nodata in finished image pass stack\n",
    "    # nothing is returned, as the ratio and dist per-pass images are modified in-place\n",
    "    print (\"A2 filled {0!s} locations on this pass\".format(gotPixelVals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=/openmp --link-args=/openmp --force\n",
    "#%%cython --compile-args=-fopenmp --link-args=-fopenmp --force \n",
    "import numpy as np\n",
    "cimport cython \n",
    "from libc.math cimport sqrt\n",
    "#cdef int _SEARCH_RADIUS = 10\n",
    "from cython.parallel import prange, parallel\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "cpdef MinMaxClip(float[:,::1] dataImage, \n",
    "                 unsigned char[:,::1] flagsImage, \n",
    "                 float[:,::1] meansImage, \n",
    "                 float[:,::1] stdImage, \n",
    "                 unsigned char flagToCheck, \n",
    "                 unsigned char flagToSet,\n",
    "                 float floor_ceiling_value,\n",
    "                 float _NDV,\n",
    "                 float upperHardLimit,\n",
    "                 float lowerHardLimit\n",
    "                 ):\n",
    "    '''\n",
    "    Clips (clamps) an image to not exceed +- n std from the mean image, or a hard upper / lower limit\n",
    "    \n",
    "    '''\n",
    "    cdef:\n",
    "        Py_ssize_t x, y, xShape, yShape\n",
    "        float value, minAllowed, maxAllowed\n",
    "        #char localFlag\n",
    "        float negInf, posInf\n",
    "        \n",
    "    yShape = dataImage.shape[0]\n",
    "    xShape = dataImage.shape[1]\n",
    "    assert xShape == meansImage.shape[1]\n",
    "    assert xShape == stdImage.shape[1]\n",
    "    assert xShape == flagsImage.shape[1]\n",
    "    assert yShape == meansImage.shape[0]\n",
    "    assert yShape == stdImage.shape[0]\n",
    "    assert yShape == flagsImage.shape[0]\n",
    "    \n",
    "    posInf = np.inf\n",
    "    negInf = -np.inf\n",
    "      \n",
    "    #with nogil, parallel(num_threads=20):\n",
    "    if 1:\n",
    "        for y in range(yShape):\n",
    "            value = -1\n",
    "            maxAllowed = posInf\n",
    "            minAllowed = negInf\n",
    "            for x in range(xShape):\n",
    "                if (flagsImage[y, x] & flagToCheck) != flagToCheck:\n",
    "                    continue\n",
    "                value = dataImage[y,x]\n",
    "                if value == _NDV:\n",
    "                    continue\n",
    "                maxAllowed = meansImage[y, x] + (floor_ceiling_value * stdImage[y, x])\n",
    "                minAllowed = meansImage[y, x] - (floor_ceiling_value * stdImage[y, x])\n",
    "                if maxAllowed>=200.0:\n",
    "                    print (\"Whoops! Location {0!s},{1!s} had value {2!s}. Mean={3!s} and std={4!s} giving maxallowed of {5!s}\"\n",
    "                           .format(x,y,value,meansImage[y,x],stdImage[y,x],maxAllowed)\n",
    "                    )\n",
    "                    # crash\n",
    "                    assert False\n",
    "                if minAllowed<=-200.0:\n",
    "                    print (\"Whoops! Location {0!s},{1!s} had value {2!s}. Mean={3!s} and std={4!s} giving minallowed of {5!s}\"\n",
    "                           .format(x,y,value,meansImage[y,x],stdImage[y,x],minAllowed)\n",
    "                    )\n",
    "                    # crash\n",
    "                    assert False\n",
    "                if maxAllowed > upperHardLimit:\n",
    "                    maxAllowed = upperHardLimit\n",
    "                if minAllowed < lowerHardLimit:\n",
    "                    minAllowed = lowerHardLimit\n",
    "                \n",
    "                if value > maxAllowed:\n",
    "                    dataImage[y, x] = maxAllowed\n",
    "                    flagsImage[y, x] = flagsImage[y, x] | flagToSet\n",
    "                    continue\n",
    "                if value < minAllowed:\n",
    "                    dataImage[y, x] = minAllowed\n",
    "                    flagsImage[y, x] = flagsImage[y, x] | flagToSet\n",
    "                    continue\n",
    "\n",
    "cpdef MinMaxClip3D(float[:,:,::1] dataImage, \n",
    "                 unsigned char[:,:,::1] flagsImage, \n",
    "                 float[:,::1] meansImage, \n",
    "                 float[:,::1] stdImage, \n",
    "                 unsigned char flagToCheck, \n",
    "                 unsigned char flagToSet,\n",
    "                 float floor_ceiling_value,\n",
    "                 float _NDV,\n",
    "                 float upperHardLimit,\n",
    "                 float lowerHardLimit\n",
    "                 ):\n",
    "    '''\n",
    "    Clips / clamps a stack of images to not exceed +- n stds from a mean image or a hard upper/lower limit\n",
    "    '''\n",
    "    cdef:\n",
    "        Py_ssize_t zSize, z\n",
    "    zSize = dataImage.shape[0]\n",
    "    assert zSize == flagsImage.shape[0]\n",
    "    for z in range(zSize):\n",
    "        MinMaxClip(dataImage[z], flagsImage[z], \n",
    "                   meansImage, stdImage, \n",
    "                   flagToCheck, flagToSet, floor_ceiling_value, _NDV, upperHardLimit, lowerHardLimit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
