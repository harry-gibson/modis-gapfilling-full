{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code used in developing and applying a topographic occlusion correct to EVI and other BRDF imagery. \n",
    "\n",
    "We observed anomalous EVI values in steep mountainous valleys, running generally east-west, in higher latitudes in winter months. We believe these are due to some areas being in permanent or near-permanent shadow under these conditions. \n",
    "\n",
    "We applied a two-stage identification process, operating on the monthly means dataset (one image for all Januaries, all Februaries, etc):\n",
    "* Identify areas likely to be affected by running a hillshade analysis with appropriate sun positions (due south in northern hemisphere and vice versa), and select pixels with illumination below an empiricially-determined threshold\n",
    "* Identify pixels  where mean EVI values in winter were higher than those in \"shoulder months\" e.g. April\n",
    "\n",
    "The pixels selected by both of these processes were identified. We then applied the following adjustment to those pixels:\n",
    "* Where the mean for e.g. January was higher than the corresponding shoulder-season month (e.g. April) then set the January value to be equal to the April value\n",
    "* Adjust the synoptic (all-images) standard deviation value for that pixel to exclude the e.g. January value and double-count the e.g. April value.\n",
    "\n",
    "Gapfilling was then run as normal using the adjusted mean and standard deviation images, such that pixels from the 8-daily imagery affected by the occlusion issue would then be more likely to be removed by the despeckle algorithm (as the net effect of the topographic correction will have been to lower the mean and reduce the standard deviation of those pixels, thus higher 8-daily pixels would be more likely to be more than 1.96 s.d. from the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 13 band images, one for each month plus one synoptic\n",
    "inMeansFile = r'G:\\NewStats\\EVI_Monthly_Means.tif'\n",
    "inSDFile = r'G:\\NewStats\\RepeatMonthlySDs\\EVI_Monthly_SDs.tif'\n",
    "inCountFile = r'G:\\NewStats\\EVI_Monthly_Counts.tif'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on identifying occluded areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two hillshade images were calculated: one for use in the northern hemisphere, with illumination from 180 deg due south; \n",
    "the other for use in the southern hemisphere with illumination from 360 deg due north. These are byte type, values from 0-255.\n",
    "\n",
    "* These were calculated on 500m (15 arcsec) data and then aggregated to 1km taking the minimum, i.e. least illuminated, of the 4 input cells.\n",
    "\n",
    "* For the areas N/S of +- 30 deg latitude, we mark as possibly occluded those parts of the respective hillshades with illumination\n",
    "below a certain threshold... empirically 120 seems about right. \n",
    "\n",
    "* The flagged occluded pixels were then expanded using a binary dilation procedure, to expand the area where the check could run. (Noting that not all pixels marked as occluded will be modified: only if they also have winter > summer means.)\n",
    "\n",
    "* Combine these into a single raster and output it. We will use this to mask the areas where the mean seasonality check runs, so as not \n",
    "to false-positive in large areas where winter rain actually does green things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hillshade reprocessed to binary image identifying pixels above/below an empirically determined threshold\n",
    "inOcclusionFile = r'C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS_Comb_LT120.tif'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the northern and southern threshold images\n",
    "\n",
    "occlusionThreshold = 120 # 95 gets turkmenistan stuff but not smaller mountatins like alaska and alps\n",
    "\n",
    "nLimit = 7200 # 30 deg north\n",
    "sLimit = 14400 # 30 deg south\n",
    "\n",
    "inHillShadeNorthFile = r'C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS180Deg_1k_5km_Min.tif'\n",
    "inHillShadeSouthFile = r'C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS360Deg_1k_5km_Min.tif'\n",
    "inHSNorthDS = gdal.Open(inHillShadeNorthFile)\n",
    "inHSNorthBand = inHSNorthDS.GetRasterBand(1)\n",
    "inHSNorth = inHSNorthBand.ReadAsArray()\n",
    "inHSSouthDS = gdal.Open(inHillShadeSouthFile)\n",
    "inHSSouthBand = inHSSouthDS.GetRasterBand(1)\n",
    "inHSSouth = inHSSouthBand.ReadAsArray()\n",
    "\n",
    "inHSNorth = inHSNorth < occlusionThreshold\n",
    "inHSSouth = inHSSouth < occlusionThreshold\n",
    "\n",
    "occludedAreas = np.zeros(shape = inHSNorth.shape, dtype = np.byte)\n",
    "occludedAreas[0:nLimit] = inHSNorth[0:nLimit]\n",
    "occludedAreas[sLimit:] = inHSSouth[sLimit:]\n",
    "\n",
    "outDrv = gdal.GetDriverByName('GTiff')\n",
    "outputOcclusion = outDrv.Create('C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS_Comb_LT{0!s}.tif'\n",
    "                                .format(occlusionThreshold),\n",
    "                        43200,21600,1,gdal.GDT_Byte,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "outputOcclusion.SetGeoTransform(inHSNorthDS.GetGeoTransform())\n",
    "outputOcclusion.SetProjection(inHSNorthDS.GetProjection())\n",
    "\n",
    "outBand = outputOcclusion.GetRasterBand(1)\n",
    "outBand.WriteArray(occludedAreas)\n",
    "outBand.FlushCache()\n",
    "outBand = None\n",
    "outputOcclusion = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with rasterio.open(inMeansFile) as src:\n",
    "    above30N = int((90 - 30) / src.res[0])\n",
    "    below30S = int((90 + 30) / src.res[0])\n",
    "    w60 = int(60 / src.res[1])\n",
    "    w90 = int(90 / src.res[1])\n",
    "    xSize = src.width\n",
    "src = gdal.Open(inMeansFile)\n",
    "globalGT = src.GetGeoTransform()\n",
    "globalProj = src.GetProjection()\n",
    "NDV = src.GetRasterBand(1).GetNoDataValue()\n",
    "src = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "struct_8 = scipy.ndimage.generate_binary_structure(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inOcclusionds = gdal.Open(inOcclusionFile)\n",
    "#occludedAreas = inOcclusionds.ReadAsArray()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inOcclusionds.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occludedAreas_Nbr1 = scipy.ndimage.binary_dilation(occludedAreas, structure=struct_8).astype(occludedAreas.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDrv = gdal.GetDriverByName('GTiff')\n",
    "outputOcclusion = outDrv.Create('C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS_Comb_LT120_Nbr1.tif',\n",
    "                        43200,21600,1,gdal.GDT_Byte,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "outputOcclusion.SetGeoTransform(globalGT)\n",
    "outputOcclusion.SetProjection(globalProj)\n",
    "\n",
    "outBand = outputOcclusion.GetRasterBand(1)\n",
    "outBand.WriteArray(occludedAreas_Nbr1)\n",
    "outBand.FlushCache()\n",
    "outBand = None\n",
    "outputOcclusion = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occludedAreas_Nbr2 = scipy.ndimage.binary_dilation(occludedAreas_Nbr1, structure=struct_8).astype(occludedAreas.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occludedAreas_Nbr2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDrv = gdal.GetDriverByName('GTiff')\n",
    "outputOcclusion = outDrv.Create('C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS_Comb_LT120_Nbr2.tif',\n",
    "                        43200,21600,1,gdal.GDT_Byte,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "outputOcclusion.SetGeoTransform(globalGT)\n",
    "outputOcclusion.SetProjection(globalProj)\n",
    "\n",
    "outBand = outputOcclusion.GetRasterBand(1)\n",
    "outBand.WriteArray(occludedAreas_Nbr2)\n",
    "outBand.FlushCache()\n",
    "outBand = None\n",
    "outputOcclusion = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inOcclusionFile_Nbr2 = r'C:\\Users\\zool1301\\Documents\\Other_Data\\Ferranti_Elev_15Sec\\Hillshade_Resample\\HS_Comb_LT120_Nbr2.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 14400)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "above30N, below30S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xCorners = np.linspace(0,xSize,6).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the occlusion dataset produced above, the identification and modification process proceeded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "suitableWidth = 4320\n",
    "edges = np.arange(0, xSize, suitableWidth)\n",
    "slices = zip(edges[:-1], edges[1:])\n",
    "slices[-1] = (slices[-1][0], xSize)\n",
    "\n",
    "#globalGT = inMeansds.GetGeoTransform()\n",
    "#globalProj = inMeansds.GetProjection()\n",
    "outDrv = gdal.GetDriverByName('GTiff')\n",
    "#ndv = bnd.GetNoDataValue()\n",
    "\n",
    "outMeanRaster = outDrv.Create(r'C:\\Users\\zool1301\\AppData\\Local\\Temp\\EVI_Monthly_Means_WinterCut_OcclusionMasked_Nbr2.tif',\n",
    "                        43200,21600,13,gdal.GDT_Float32,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "outSDRaster = outDrv.Create(r'C:\\Users\\zool1301\\AppData\\Local\\Temp\\EVI_Monthly_SDs_WinterCut_OcclusionMasked_Nbr2.tif',\n",
    "                        43200,21600,13,gdal.GDT_Float32,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "outCountRaster = outDrv.Create(r'C:\\Users\\zool1301\\AppData\\Local\\Temp\\EVI_Monthly_Count_WinterCut_OcclusionMasked_Nbr2.tif',\n",
    "                        43200,21600,13,gdal.GDT_Int16,\n",
    "                        [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "\n",
    "flagsRaster = outDrv.Create(r'C:\\Users\\zool1301\\AppData\\Local\\Temp\\EVI_Monthly_Means_WinterCutFlags_OcclusionMasked_Nbr2.tif',\n",
    "                     43200,21600,12,gdal.GDT_Byte,\n",
    "                     [\"TILED=YES\",\"SPARSE_OK=TRUE\",\"BIGTIFF=YES\",\"INTERLEAVE=BAND\",\"COMPRESS=LZW\",\"PREDICTOR=2\"])\n",
    "\n",
    "outMeanRaster.SetGeoTransform(globalGT)\n",
    "outMeanRaster.SetProjection(globalProj)\n",
    "outSDRaster.SetGeoTransform(globalGT)\n",
    "outSDRaster.SetProjection(globalProj)\n",
    "outCountRaster.SetGeoTransform(globalGT)\n",
    "outCountRaster.SetProjection(globalProj)\n",
    "\n",
    "flagsRaster.SetGeoTransform(globalGT)\n",
    "flagsRaster.SetProjection(globalProj)\n",
    "\n",
    "for west, east in slices:\n",
    "    print \"{0!s} - {1!s} loading...\".format(west,east),\n",
    "    inMeansDS = gdal.Open(inMeansFile)\n",
    "    meanDataStack = inMeansDS.ReadAsArray(west, 0, east-west)#[:-1]\n",
    "    inMeansDS = None\n",
    "    inSdDS =  gdal.Open(inSDFile)\n",
    "    sdDataStack = inSdDS.ReadAsArray(west, 0, east-west)#[:-1]\n",
    "    inSdDS = None\n",
    "    inCountDS = gdal.Open(inCountFile)\n",
    "    countDataStack = inCountDS.ReadAsArray(west, 0, east-west)#[:-1]\n",
    "    inCountDS = None\n",
    "    #inOcclusionds = gdal.Open(inOcclusionFile)\n",
    "    inOcclusionds = gdal.Open(inOcclusionFile_Nbr2)\n",
    "    occludedAreas = inOcclusionds.ReadAsArray(west, 0, east-west)\n",
    "    inOcclusionds = None\n",
    "    \n",
    "    print \"Running...\",\n",
    "    resFlags = ApplyWinterMinCut(meanDataStack, sdDataStack, countDataStack, occludedAreas, NDV)\n",
    "    print \"Saving...\",\n",
    "    for b in range(13):\n",
    "        print \"{0!s}... \".format(b),\n",
    "        outMeanRaster.GetRasterBand(b+1).WriteArray(meanDataStack[b], west, 0)\n",
    "        outSDRaster.GetRasterBand(b+1).WriteArray(sdDataStack[b], west, 0)\n",
    "        outCountRaster.GetRasterBand(b+1).WriteArray(countDataStack[b], west, 0)\n",
    "        \n",
    "        if b < 12:\n",
    "            flagsRaster.GetRasterBand(b+1).WriteArray(resFlags[b], west, 0)\n",
    "        \n",
    "        outMeanRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "        outSDRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "        outCountRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "        \n",
    "        if b < 12:\n",
    "            flagsRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "        \n",
    "        #outputRaster.GetRasterBand(b+1).WriteArray(res[0][b], west, 0)\n",
    "        #flagsRaster.GetRasterBand(b+1).WriteArray(res[1][b], west, 0)\n",
    "        #outputRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "        #flagsRaster.GetRasterBand(b+1).SetNoDataValue(NDV)\n",
    "    print \n",
    "flagsRaster.FlushCache()\n",
    "outMeanRaster.FlushCache()\n",
    "outSDRaster.FlushCache()\n",
    "outCountRaster.FlushCache()\n",
    "\n",
    "flagsRaster = None\n",
    "outMeanRaster = None\n",
    "outSDRaster = None\n",
    "outCountRaster = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "cpdef RegenerateGrandStats(float[:] groupMeans, float[:] groupSDs, short[:] groupCounts, float NDV):\n",
    "    '''\n",
    "    Recalculates \"grand standard deviation\" (and mean) from the group SDs, means, and counts. \n",
    "    \n",
    "    For use to recalculate \"SD_From_Daily\" when we have modified some of the \"Monthly_SDs\" \n",
    "    e.g. decided to exclude the January SD value at a particular location if it was shaded. \n",
    "    \n",
    "    In such a case we need to recalculate the SD that was produced from all daily values, \n",
    "    to also reflect the exclusion of the January days, but don't want to go back through all the \n",
    "    daily data to do so.    \n",
    "    \n",
    "    See\n",
    "    http://www.burtonsys.com/climate/composite_sd.php#python\n",
    "    '''\n",
    "    \n",
    "    assert groupMeans.shape[0] == groupSDs.shape[0]\n",
    "    assert groupMeans.shape[0] == groupCounts.shape[0]\n",
    "    \n",
    "    cdef:\n",
    "        Py_ssize_t length, i\n",
    "        int N = 0\n",
    "        double GM = 0.0\n",
    "        \n",
    "        double ESS = 0.0\n",
    "        double TGSS = 0.0\n",
    "        \n",
    "        double G_STD = 0.0\n",
    "        \n",
    "    length = groupMeans.shape[0]\n",
    "    \n",
    "    for i in range(length):\n",
    "        if groupCounts[i] != 0 and groupMeans[i] != NDV:\n",
    "            N += groupCounts[i]\n",
    "            GM += groupMeans[i] * groupCounts[i]\n",
    "            ESS += ((groupSDs[i])**2) * (groupCounts[i]-1)\n",
    "    \n",
    "    #print \"group mean sum {0!s}\".format(GM)\n",
    "    if N > 1:\n",
    "        GM = GM / N\n",
    "    #print \"GM {0!s}\".format(GM)\n",
    "    \n",
    "    for i in range(length):\n",
    "        if groupCounts[i] != 0 and groupMeans[i] != NDV:\n",
    "            TGSS += ((groupMeans[i] - GM)**2) * groupCounts[i]\n",
    "    \n",
    "    if N > 1:\n",
    "        G_STD = sqrt((ESS+TGSS) / (N-1))\n",
    "    \n",
    "    return (GM, G_STD, N)\n",
    "\n",
    "\n",
    "cpdef ApplyWinterMinCut(float[:,:,::1] meansData, float[:,:,::1] sdData, short[:,:,::1] countData,\n",
    "                        char[:,::1] occludedAreas, float NDV):\n",
    "\n",
    "    '''\n",
    "    Applies topographic occlusion \"correction\" as described in method notes.\n",
    "    \n",
    "    Filters monthly mean / standard deviation / count values to remove pixels that are higher \n",
    "    in winter than in summer, if those pixels are also marked as being heavily shaded.\n",
    "    \n",
    "    Designed for use with EVI mean data to overcome the problem of erroneous high values \n",
    "    in shaded mountain valleys in winter.\n",
    "    \n",
    "    Values removed are replaced with those from the corresponding \"shoulder season\" month, \n",
    "    whichever is lowest (april or september in north, october or march in the south),\n",
    "    and the overall standard deviation / mean / count (in band 13) is updated to reflect \n",
    "    the changes.\n",
    "    '''\n",
    "    \n",
    "    cdef:\n",
    "        Py_ssize_t above30N, below30S, y, x, yShape, xShape, zShape\n",
    "        \n",
    "        float meanApr, meanSep, meanOct, meanMar\n",
    "        float sdApr, sdSep, sdOct, sdMar\n",
    "        int countApr, countSep, countOct, countMar\n",
    "        float repMean, repSD\n",
    "        int repCount\n",
    "        char repSrcMnth\n",
    "        \n",
    "        char[:,:,::1] flags\n",
    "        long long numChanged = 0\n",
    "        char locationModified = 0\n",
    "        \n",
    "        float[:] locationMeans\n",
    "        float[:] locationSDs\n",
    "        short[:] locationCounts\n",
    "    \n",
    "    flags = np.zeros(shape=(meansData.shape[0],meansData.shape[1],meansData.shape[2]), dtype = np.uint8)\n",
    "    \n",
    "    locationMeans = np.zeros(shape=(meansData.shape[0]-1), dtype = np.float32)\n",
    "    locationSDs = np.zeros(shape=(meansData.shape[0]-1), dtype = np.float32)\n",
    "    locationCounts = np.zeros(shape=(meansData.shape[0]-1), dtype = np.int16)\n",
    "    \n",
    "    yShape = meansData.shape[1]\n",
    "    xShape = meansData.shape[2]\n",
    "    zShape = meansData.shape[0]\n",
    "    print (\"{0!s},{1!s},{2!s}\".format(zShape,yShape,xShape))\n",
    "    above30N = 7200\n",
    "    below30S = 14400\n",
    "    \n",
    "    assert meansData.shape[1] == occludedAreas.shape[0]\n",
    "    assert meansData.shape[2] == occludedAreas.shape[1]\n",
    "    assert countData.shape[1] == occludedAreas.shape[0]\n",
    "    assert countData.shape[2] == occludedAreas.shape[1]\n",
    "    assert sdData.shape[1] == occludedAreas.shape[0]\n",
    "    assert sdData.shape[2] == occludedAreas.shape[1]\n",
    "    assert meansData.shape[0] == countData.shape[0]\n",
    "    assert meansData.shape[0] == sdData.shape[0]\n",
    "    \n",
    "    assert NDV < -90\n",
    "    \n",
    "    for y in range (yShape):\n",
    "        if y > above30N and y < below30S:\n",
    "                # don't do anything within 30deg of equator\n",
    "                continue\n",
    "        if y <= above30N:\n",
    "            for x in range (xShape):\n",
    "                \n",
    "                if occludedAreas[y, x] == 0:\n",
    "                    # don't do anything if we're not on a bit that is predicted to be shadowed\n",
    "                    continue\n",
    "                \n",
    "                locationModified = 0\n",
    "                \n",
    "                # Determine the threshold that we will compare winter months to to see if they are \n",
    "                # unrealistic\n",
    "                \n",
    "                # north of 30N get the min value as lower of april and september\n",
    "                meanApr = meansData[3, y, x]\n",
    "                meanSep = meansData[8, y, x]\n",
    "                sdApr = sdData[3, y, x]\n",
    "                sdSep = sdData[8, y, x]\n",
    "                countApr = countData[3, y, x]\n",
    "                countSep = countData[8, y, x]\n",
    "                \n",
    "                # replace ND with a large value so it won't pick up in the less-than test\n",
    "                if meanApr == NDV:\n",
    "                    meanApr = 9999\n",
    "                if meanSep == NDV:\n",
    "                    meanSep = 9999\n",
    "\n",
    "                if meanApr < meanSep:\n",
    "                    # april is lower OR sept is nodata\n",
    "                    repMean = meanApr\n",
    "                    repSD = sdApr\n",
    "                    repCount = countApr\n",
    "                    repSrcMnth = 4\n",
    "                \n",
    "                elif meanSep < meanApr:\n",
    "                    # sept is lower OR apr is nodata\n",
    "                    repMean = meanSep\n",
    "                    repSD = sdSep\n",
    "                    repCount = countSep\n",
    "                    repSrcMnth = 9\n",
    "                    \n",
    "                elif meanApr == 9999:\n",
    "                    # both are no data \n",
    "                    repMean = 0\n",
    "                    repSD = 0\n",
    "                    repCount = 0\n",
    "                    repSrcMnth = -13\n",
    "                    \n",
    "                else:\n",
    "                    # neither is nodata but they are equal\n",
    "                    repMean = meanApr\n",
    "                    # we will assume in this case, although it's not mathematically definite,\n",
    "                    # that the SDs and counts are also the same\n",
    "                    repSD = sdApr\n",
    "                    repCount = countApr\n",
    "                    repSrcMnth = 13\n",
    "                \n",
    "                # Now apply this threshold to oct, nov, dec, jan, feb, mar\n",
    "                if meansData[0, y, x] > repMean:\n",
    "                    meansData[0, y, x] = repMean\n",
    "                    sdData[0, y, x] = repSD\n",
    "                    countData[0, y, x] = repCount\n",
    "                    flags [0, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData[1, y, x] > repMean:\n",
    "                    meansData[1,y, x] = repMean\n",
    "                    sdData[1, y, x] = repSD\n",
    "                    countData[1, y, x] = repCount\n",
    "                    flags [1, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData[2, y, x] > repMean:\n",
    "                    meansData[2, y, x] = repMean\n",
    "                    sdData[2, y, x] = repSD\n",
    "                    countData[2, y, x] = repCount\n",
    "                    flags [2, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData[9, y, x] > repMean:\n",
    "                    meansData[9, y, x] = repMean\n",
    "                    sdData[9, y, x] = repSD\n",
    "                    countData[9, y, x] = repCount\n",
    "                    flags [9, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData[10, y, x] > repMean:\n",
    "                    meansData[10, y, x] = repMean\n",
    "                    sdData[10, y, x] = repSD\n",
    "                    countData[10, y, x] = repCount\n",
    "                    flags [10, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData[11, y, x] > repMean:\n",
    "                    meansData[11, y, x] = repMean\n",
    "                    sdData[11, y, x] = repSD\n",
    "                    countData[11, y, x] = repCount\n",
    "                    flags [11, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                \n",
    "                if locationModified == 1:\n",
    "                    locationMeans = meansData[0:zShape-1, y, x]\n",
    "                    locationSDs = sdData[0:zShape-1, y, x]\n",
    "                    locationCounts = countData[0:zShape-1, y, x]\n",
    "                    \n",
    "                    newAllTimeDailyVals = RegenerateGrandStats(locationMeans, locationSDs, locationCounts, NDV)\n",
    "                    meansData[zShape-1, y, x] = newAllTimeDailyVals[0]\n",
    "                    sdData[zShape-1, y, x] = newAllTimeDailyVals[1]\n",
    "                    countData[zShape-1, y, x] = newAllTimeDailyVals[2]\n",
    "\n",
    "        elif y >= below30S:\n",
    "            for x in range (xShape):\n",
    "                if occludedAreas[y, x] == 0:\n",
    "                    continue\n",
    "                \n",
    "                locationModified = 0\n",
    "                \n",
    "                meanOct = meansData[9, y, x]\n",
    "                meanMar = meansData[2, y, x]\n",
    "                sdOct = sdData[9, y, x]\n",
    "                sdMar = sdData[2, y, x]\n",
    "                countOct = countData[9, y, x]\n",
    "                countMar = countData[2, y, x]\n",
    "                \n",
    "                if meanOct == NDV:\n",
    "                    meanOct = 9999\n",
    "                if meanMar == NDV:\n",
    "                    meanMar = 9999\n",
    "                \n",
    "                if meanOct < meanMar:\n",
    "                    repMean = meanOct\n",
    "                    repSD = sdOct\n",
    "                    repCount = countOct\n",
    "                    repSrcMnth = 10\n",
    "                    \n",
    "                elif meanMar < meanOct:\n",
    "                    repMean = meanMar\n",
    "                    repSD = sdMar\n",
    "                    repCount = countMar\n",
    "                    repSrcMnth = 3\n",
    "                    \n",
    "                elif meanOct == 9999:\n",
    "                    repMean = 0\n",
    "                    repSD = 0\n",
    "                    repCount = 0\n",
    "                    repSrcMnth = -13\n",
    "                    \n",
    "                else:\n",
    "                    repMean = meanOct\n",
    "                    repSD = sdOct\n",
    "                    repCount = countOct\n",
    "                    repSrcMnth = 13\n",
    "                    \n",
    "                # apply this threshold to apr, may, jun, jul, aug, sep    \n",
    "                if meansData[3, y, x] > repMean:\n",
    "                    meansData[3, y, x] = repMean\n",
    "                    sdData[3, y ,x] = repSD\n",
    "                    countData[3, y, x] = repCount\n",
    "                    flags [3, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData [4, y, x] > repMean:\n",
    "                    meansData[4, y, x] = repMean\n",
    "                    sdData[4, y ,x] = repSD\n",
    "                    countData[4, y, x] = repCount\n",
    "                    flags [4, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData [5, y, x] > repMean:\n",
    "                    meansData[5, y, x] = repMean\n",
    "                    sdData[5, y ,x] = repSD\n",
    "                    countData[5, y, x] = repCount\n",
    "                    flags [5, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData [6, y, x] > repMean:\n",
    "                    meansData[6, y, x] = repMean\n",
    "                    sdData[6, y ,x] = repSD\n",
    "                    countData[6, y, x] = repCount\n",
    "                    flags [6, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData [7, y, x] > repMean:\n",
    "                    meansData[7, y, x] = repMean\n",
    "                    sdData[7, y ,x] = repSD\n",
    "                    countData[7, y, x] = repCount\n",
    "                    flags [7, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                if meansData [8, y, x] > repMean:\n",
    "                    meansData[8, y, x] = repMean\n",
    "                    sdData[8, y ,x] = repSD\n",
    "                    countData[8, y, x] = repCount\n",
    "                    flags [8, y, x] = repSrcMnth\n",
    "                    locationModified = 1\n",
    "                    numChanged += 1\n",
    "                \n",
    "                if locationModified == 1:\n",
    "                    locationMeans = meansData[0:zShape-1, y, x]\n",
    "                    locationSDs = sdData[0:zShape-1, y, x]\n",
    "                    locationCounts = countData[0:zShape-1, y, x]\n",
    "                    \n",
    "                    newAllTimeDailyVals = RegenerateGrandStats(locationMeans, locationSDs, locationCounts, NDV)\n",
    "                    meansData[zShape-1, y, x] = newAllTimeDailyVals[0]\n",
    "                    sdData[zShape-1, y, x] = newAllTimeDailyVals[1]\n",
    "                    countData[zShape-1, y, x] = newAllTimeDailyVals[2]\n",
    "\n",
    "    print \"Modified {0!s} cell values\".format(numChanged)\n",
    "    #return (np.asarray(meansData), np.asarray(sdData), np.asarray(countData), np.asarray(flags))\n",
    "    return np.asarray(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell is not used: it was an earlier version of the wintermincut function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xCorners = np.linspace(0,xSize,10).astype(np.int32)\n",
    "#yCorners = np.linspace(0,ySize,6).astype(np.int32)\n",
    "meanVals = np.empty(shape=(ySize, xSize),dtype=np.float32)\n",
    "countVals = np.empty(shape=(ySize, xSize),dtype='byte')\n",
    "\n",
    "for x in xrange(len(xCorners)-1):\n",
    "    x0 = xCorners[x]\n",
    "    x1 = xCorners[x+1]\n",
    "    #y0 = yCorners[y]\n",
    "    #y1 = yCorners[y+1]\n",
    "    #dataStack = src.read(window=((0,ySize),(x0,x1)), masked=True)\n",
    "    dataStack= inMeansds.ReadAsArray(x0,0,x1-x0)[:-1]\n",
    "    #dataStack = np.ma.MaskedArray(dataStackRaw, dataStackRaw == NDV)\n",
    "    #dataStack[dataStack == NDV] = np.nan\n",
    "    \n",
    "    janN = dataStack[0, 0:above30N, :]\n",
    "    febN = dataStack[1, 0:above30N, :]\n",
    "    marN = dataStack[2, 0:above30N, :]\n",
    "    octN = dataStack[9, 0:above30N, :]\n",
    "    novN = dataStack[10, 0:above30N, :]\n",
    "    decN = dataStack[11, 0:above30N, :]\n",
    "    \n",
    "    # generate a threshold grid as the minimum of apr and sept \n",
    "    # Where one of these is nodata, use the other, but when both \n",
    "    # are nodata set the output to zero\n",
    "    # Make copies as we don't want to change the input \n",
    "    aprN = np.copy(dataStack[3, 0:above30N, :])\n",
    "    sepN = np.copy(dataStack[8, 0:above30N, :])\n",
    "    # minimum ignoring nans unless both are nan\n",
    "    aprN[aprN == NDV] = np.nan\n",
    "    sepN[sepN == NDV] = np.nan\n",
    "    threshN = np.fmin(aprN, sepN)\n",
    "    #threshN[np.isnan(threshN)] = 0\n",
    "    threshN = np.nan_to_num(threshN)\n",
    "\n",
    "    aprS = dataStack[3, below30S:, :]\n",
    "    mayS = dataStack[4, below30S:, :]\n",
    "    junS = dataStack[5, below30S:, :]\n",
    "    julS = dataStack[6, below30S:, :]\n",
    "    augS = dataStack[7, below30S:, :]\n",
    "    sepS = dataStack[8, below30S:, :]\n",
    "    \n",
    "    octS = dataStack[9, below30S:, :]\n",
    "    marS = dataStack[2, below30S:, :]\n",
    "    threshS = np.fmin(octS, marS)\n",
    "    #threshS[np.isnan(threshS)] = 0\n",
    "    threshS = np.nan_to_num(threshS)\n",
    "    \n",
    "    meanTile = meanVals[:,x0:x1]\n",
    "    countTile = countVals[:,x0:x1]\n",
    "    janN[janN > aprN] = 0\n",
    "    febN[febN > aprN] = 0\n",
    "    marN[marN > aprN] = 0\n",
    "    octN[octN > sepN] = 0\n",
    "    novN[novN > sepN] = 0\n",
    "    decN[decN > sepN] = 0\n",
    "    \n",
    "    mayS[mayS > aprS] = 0\n",
    "    junS[junS > aprS] = 0\n",
    "    julS[julS > sepS] = 0\n",
    "    augS[augS > sepS] = 0\n",
    "    \n",
    "    meanTile[:] = np.ma.mean(dataStack[0:12],axis=0)\n",
    "    countTile[:] = np.logical_not(dataStack.mask).sum(axis=0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
